<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>FAQ · Enzyme.jl</title><meta name="title" content="FAQ · Enzyme.jl"/><meta property="og:title" content="FAQ · Enzyme.jl"/><meta property="twitter:title" content="FAQ · Enzyme.jl"/><meta name="description" content="Documentation for Enzyme.jl."/><meta property="og:description" content="Documentation for Enzyme.jl."/><meta property="twitter:description" content="Documentation for Enzyme.jl."/><meta property="og:url" content="https://enzyme.mit.edu/julia/faq/"/><meta property="twitter:url" content="https://enzyme.mit.edu/julia/faq/"/><link rel="canonical" href="https://enzyme.mit.edu/julia/faq/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><script src="https://plausible.io/js/plausible.js" data-domain="enzyme.mit.edu" defer></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Enzyme.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Enzyme.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../generated/autodiff/">Basics</a></li><li><a class="tocitem" href="../generated/box/">Box model</a></li><li><a class="tocitem" href="../generated/custom_rule/">Custom rules</a></li></ul></li><li class="is-active"><a class="tocitem" href>FAQ</a><ul class="internal"><li><a class="tocitem" href="#Implementing-pullbacks"><span>Implementing pullbacks</span></a></li><li><a class="tocitem" href="#Identical-types-in-Duplicated-/-Memory-Layout"><span>Identical types in <code>Duplicated</code> / Memory Layout</span></a></li><li><a class="tocitem" href="#CUDA-support"><span>CUDA support</span></a></li><li><a class="tocitem" href="#Linear-Algebra"><span>Linear Algebra</span></a></li><li><a class="tocitem" href="#Sparse-arrays"><span>Sparse arrays</span></a></li><li><a class="tocitem" href="#Activity-of-temporary-storage"><span>Activity of temporary storage</span></a></li><li><a class="tocitem" href="#Runtime-Activity"><span>Runtime Activity</span></a></li><li><a class="tocitem" href="#Mixed-activity"><span>Mixed activity</span></a></li><li><a class="tocitem" href="#Complex-numbers"><span>Complex numbers</span></a></li><li><a class="tocitem" href="#What-types-are-differentiable?"><span>What types are differentiable?</span></a></li></ul></li><li><a class="tocitem" href="../api/">API reference</a></li><li><span class="tocitem">Advanced</span><ul><li><a class="tocitem" href="../dev_docs/">For developers</a></li><li><a class="tocitem" href="../internal_api/">Internal API</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>FAQ</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>FAQ</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/EnzymeAD/Enzyme.jl/blob/main/docs/src/faq.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Frequently-asked-questions"><a class="docs-heading-anchor" href="#Frequently-asked-questions">Frequently asked questions</a><a id="Frequently-asked-questions-1"></a><a class="docs-heading-anchor-permalink" href="#Frequently-asked-questions" title="Permalink"></a></h1><h2 id="Implementing-pullbacks"><a class="docs-heading-anchor" href="#Implementing-pullbacks">Implementing pullbacks</a><a id="Implementing-pullbacks-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-pullbacks" title="Permalink"></a></h2><p>In combined reverse mode, Enzyme&#39;s <a href="../api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{RuntimeActivity}, Tuple{ErrIfFuncWritten}, Tuple{Nargs}, Tuple{RABI}, Tuple{ReturnPrimal}, Tuple{ForwardMode{ReturnPrimal, RABI, ErrIfFuncWritten, RuntimeActivity}, FA, Type{A}, Vararg{Annotation, Nargs}}} where {ReturnPrimal, RABI&lt;:EnzymeCore.ABI, Nargs, ErrIfFuncWritten, RuntimeActivity, FA&lt;:Annotation, A&lt;:Annotation}"><code>autodiff</code></a> function can only handle functions with scalar output (this is not true for split reverse mode, aka <code>autodiff_thunk</code>). To implement pullbacks (back-propagation of gradients/tangents) for array-valued functions, use a mutating function that returns <code>nothing</code> and stores its result in one of the arguments, which must be passed wrapped in a <a href="../api/#EnzymeCore.Duplicated"><code>Duplicated</code></a>. Regardless of AD mode, this mutating function will be much more efficient anyway than one which allocates the output.</p><p>Given a function <code>mymul!</code> that performs the equivalent of <code>R = A * B</code> for matrices <code>A</code> and <code>B</code>, and given a gradient (tangent) <code>∂z_∂R</code>, we can compute <code>∂z_∂A</code> and <code>∂z_∂B</code> like this:</p><pre><code class="language-julia hljs">using Enzyme, Random

function mymul!(R, A, B)
    @assert axes(A,2) == axes(B,1)
    @inbounds @simd for i in eachindex(R)
        R[i] = 0
    end
    @inbounds for j in axes(B, 2), i in axes(A, 1)
        @inbounds @simd for k in axes(A,2)
            R[i,j] += A[i,k] * B[k,j]
        end
    end
    nothing
end

Random.seed!(1234)
A = rand(5, 3)
B = rand(3, 7)

R = zeros(size(A,1), size(B,2))
∂z_∂R = rand(size(R)...)  # Some gradient/tangent passed to us
∂z_∂R0 = copyto!(similar(∂z_∂R), ∂z_∂R)  # exact copy for comparison

∂z_∂A = zero(A)
∂z_∂B = zero(B)

Enzyme.autodiff(Reverse, mymul!, Const, Duplicated(R, ∂z_∂R), Duplicated(A, ∂z_∂A), Duplicated(B, ∂z_∂B))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((nothing, nothing, nothing),)</code></pre><p>Now we have:</p><pre><code class="language-julia hljs">R ≈ A * B            &amp;&amp;
∂z_∂A ≈ ∂z_∂R0 * B&#39;  &amp;&amp;  # equivalent to Zygote.pullback(*, A, B)[2](∂z_∂R)[1]
∂z_∂B ≈ A&#39; * ∂z_∂R0      # equivalent to Zygote.pullback(*, A, B)[2](∂z_∂R)[2]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>Note that the result of the backpropagation is <em>added to</em> <code>∂z_∂A</code> and <code>∂z_∂B</code>, they act as accumulators for gradient information.</p><h2 id="Identical-types-in-Duplicated-/-Memory-Layout"><a class="docs-heading-anchor" href="#Identical-types-in-Duplicated-/-Memory-Layout">Identical types in <code>Duplicated</code> / Memory Layout</a><a id="Identical-types-in-Duplicated-/-Memory-Layout-1"></a><a class="docs-heading-anchor-permalink" href="#Identical-types-in-Duplicated-/-Memory-Layout" title="Permalink"></a></h2><p>Enzyme checks that <code>x</code> and <code>∂f_∂x</code> have the same types when constructing objects of type <code>Duplicated</code>, <code>DuplicatedNoNeed</code>, <code>BatchDuplicated</code>, etc. This is not a mathematical or practical requirement within Enzyme, but rather a guardrail to prevent user error. The memory locations of the shadow <code>∂f_∂x</code> can only be accessed in the derivative function <code>∂f</code> if the corresponding memory locations of the variable <code>x</code> are accessed by the function <code>f</code>. Imposing that the variable <code>x</code> and shadow <code>∂f_∂x</code> have the same type is a heuristic way to ensure that they have the same data layout. This helps prevent some user errors, for instance when the provided shadow cannot be accessed at the relevant memory locations.</p><p>In some ways, type equality is too strict: two different types can have the same data layout. For instance, a vector and a view of a matrix column are arranged identically in memory. But in other ways it is not strict enough. Suppose you have a function <code>f(x) = x[7]</code>. If you call <code>Enzyme.autodiff(Reverse, f, Duplicated(ones(10), ones(1))</code>, the type check alone will not be sufficient. Since the original code accesses <code>x[7]</code>, the derivative code will try to set <code>∂f_∂x[7]</code>. The length is not encoded in the type, so Julia cannot provide a high-level error before running <code>autodiff</code>, and the user may end up with a segfault (or other memory error) when running the generated derivative code. Another typical example is sparse arrays, for which the sparsity pattern of <code>x</code> and <code>∂f_∂x</code> should be identical.</p><p>To make sure that <code>∂f_∂x</code> has the right data layout, create it with <code>∂f_∂x = Enzyme.make_zero(x)</code>.</p><h3 id="Circumventing-Duplicated-Restrictions-/-Advanced-Memory-Layout"><a class="docs-heading-anchor" href="#Circumventing-Duplicated-Restrictions-/-Advanced-Memory-Layout">Circumventing Duplicated Restrictions / Advanced Memory Layout</a><a id="Circumventing-Duplicated-Restrictions-/-Advanced-Memory-Layout-1"></a><a class="docs-heading-anchor-permalink" href="#Circumventing-Duplicated-Restrictions-/-Advanced-Memory-Layout" title="Permalink"></a></h3><p>Advanced users may leverage Enzyme&#39;s memory semantics (only touching locations in the shadow that were touched in the primal) for additional performance/memory savings, at the obvious cost of potential safety if used incorrectly.</p><p>Consider the following function that loads from offset 47 of a Ptr</p><pre><code class="language-julia hljs">function f(ptr)
    x = unsafe_load(ptr, 47)
    x * x
end

ptr = Base.reinterpret(Ptr{Float64}, Libc.malloc(100*sizeof(Float64)))
unsafe_store!(ptr, 3.14, 47)

f(ptr)

# output
9.8596</code></pre><p>The recommended (and guaranteed sound) way to differentiate this is to pass in a shadow pointer that is congruent with the primal. That is to say, its length (and recursively for any sub types) are equivalent to the primal.</p><pre><code class="language-julia hljs">ptr = Base.reinterpret(Ptr{Float64}, Libc.malloc(100*sizeof(Float64)))
unsafe_store!(ptr, 3.14, 47)
dptr = Base.reinterpret(Ptr{Float64}, Libc.calloc(100*sizeof(Float64), 1))

autodiff(Reverse, f, Duplicated(ptr, dptr))

unsafe_load(dptr, 47)

# output
6.28</code></pre><p>However, since we know the original function only reads from one float64, we could choose to only allocate a single float64 for the shadow, as long as we ensure that loading from offset 47 (the only location accessed) is in bounds.</p><pre><code class="language-julia hljs">ptr = Base.reinterpret(Ptr{Float64}, Libc.malloc(100*sizeof(Float64)))
unsafe_store!(ptr, 3.14, 47)
dptr = Base.reinterpret(Ptr{Float64}, Libc.calloc(sizeof(Float64), 1))

# offset the pointer to have unsafe_load(dptr, 47) access the 0th byte of dptr
# since julia one indexes we subtract 46 * sizeof(Float64) here
autodiff(Reverse, f, Duplicated(ptr, dptr - 46 * sizeof(Float64)))

# represents the derivative of the 47&#39;th elem of ptr, 
unsafe_load(dptr)

# output
6.28</code></pre><p>However, this style of optimization is not specific to Enzyme, or AD, as one could have done the same thing on the primal code where it only passed in one float. The difference, here however, is that performing these memory-layout tricks safely in Enzyme requires understanding the access patterns of the generated derivative code – like discussed here.</p><pre><code class="language-julia hljs">ptr = Base.reinterpret(Ptr{Float64}, Libc.calloc(sizeof(Float64), 1))
unsafe_store!(ptr, 3.14)
# offset the pointer to have unsafe_load(ptr, 47) access the 0th byte of dptr
# again since julia one indexes we subtract 46 * sizeof(Float64) here
f(ptr - 46 * sizeof(Float64))

# output
9.8596</code></pre><h2 id="CUDA-support"><a class="docs-heading-anchor" href="#CUDA-support">CUDA support</a><a id="CUDA-support-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-support" title="Permalink"></a></h2><p><a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> is only supported on Julia v1.7.0 and onwards. On v1.6, attempting to differentiate CUDA kernel functions will not use device overloads correctly and thus returns fundamentally wrong results.</p><p>Specifically, differentiating within device kernels is supported. See our <a href="https://github.com/EnzymeAD/Enzyme.jl/blob/main/test/cuda.jl">cuda tests</a> for some examples. </p><p>Differentiating through a heterogeneous (e.g. combined host and device) code presently requires defining a custom derivative that tells Enzyme that differentiating an <code>@cuda</code> call is done by performing <code>@cuda</code> of its generated derivative. For an example of this in Enzyme-C++ see <a href="https://enzyme.mit.edu/getting_started/CUDAGuide/">here</a>. Automating this for a better experience for CUDA.jl requires an update to <a href="https://github.com/JuliaGPU/CUDA.jl/pull/1869/files">CUDA.jl</a>, and is now available for Kernel Abstractions.</p><p>Differentiating host-side code when accesses device memory (e.g. <code>sum(CuArray)</code>) is not yet supported, but in progress.</p><h2 id="Linear-Algebra"><a class="docs-heading-anchor" href="#Linear-Algebra">Linear Algebra</a><a id="Linear-Algebra-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Algebra" title="Permalink"></a></h2><p>Enzyme supports presently some, but not all of Julia&#39;s linear algebra library. This is because some of Julia&#39;s linear algebra library is not pure Julia code and calls external functions such as BLAS, LaPACK, CuBLAS, SuiteSparse, etc.</p><p>For all BLAS functions, Enzyme will generate a correct derivative function. If it is a <code>gemm</code> (matmul), <code>gemv</code> (matvec), <code>dot</code> (dot product), <code>axpy</code> (vector add and scale), and a few others, Enzyme will generate a fast derivative using another corresponding BLAS call.  For other BLAS functions, Enzyme will presently emit a warning <code>Fallback BLAS [functionname]</code> that indicates that Enzyme will differentiate this function by differentiating a serial implementation of BLAS. This will still work for all BLAS codes, but may be slower on a parallel platform.</p><p>Other libraries do not yet have derivatives (either fast or fallback) implemented within Enzyme. Supporting these is not a fundamental limitation, but requires implementing a rule in Enzyme describing how to differentiate them. Contributions welcome!</p><h2 id="Sparse-arrays"><a class="docs-heading-anchor" href="#Sparse-arrays">Sparse arrays</a><a id="Sparse-arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-arrays" title="Permalink"></a></h2><p>Differentiating code using sparse arrays is supported, but care must be taken because backing arrays drop zeros in Julia (unless told not to).</p><pre><code class="language-julia hljs">using SparseArrays
a = sparse([2.0])
da1 = sparse([0.0]) # Incorrect: SparseMatrixCSC drops explicit zeros
Enzyme.autodiff(Reverse, sum, Active, Duplicated(a, da1))
da1

# output

1-element SparseVector{Float64, Int64} with 0 stored entries</code></pre><pre><code class="language-julia hljs">da2 = sparsevec([1], [0.0]) # Correct: Prevent SparseMatrixCSC from dropping zeros
Enzyme.autodiff(Reverse, sum, Active, Duplicated(a, da2))
da2

# output

1-element SparseVector{Float64, Int64} with 1 stored entry:
  [1]  =  1.0</code></pre><p>Sometimes, determining how to perform this zeroing can be complicated. That is why Enzyme provides a helper function <code>Enzyme.make_zero</code> that does this automatically.</p><pre><code class="language-julia hljs">Enzyme.make_zero(a)
Enzyme.gradient(Reverse, sum, a)[1] # This calls make_zero(a)

# output

1-element SparseVector{Float64, Int64} with 1 stored entry:
  [1]  =  1.0</code></pre><p>Some Julia libraries sparse linear algebra libraries call out to external C code like SuiteSparse which we don&#39;t presently implement derivatives for (we have some but have yet to complete all). If that case happens, Enzyme will throw a &quot;no derivative found&quot; error at the callsite of that function. This isn&#39;t a fundamental limitation, and is easily resolvable by writing a custom rule or internal Enzyme support. Help is certainly welcome :).</p><h3 id="Advanced-Sparse-arrays"><a class="docs-heading-anchor" href="#Advanced-Sparse-arrays">Advanced Sparse arrays</a><a id="Advanced-Sparse-arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Sparse-arrays" title="Permalink"></a></h3><p>Essentially the way Enzyme represents all data structures, including sparse data structures, is to have the shadow (aka derivative) memory be the same memory layout as the primal. Suppose you have an input data structure <code>x</code>. The derivative of <code>x</code> at byte offset 12 will be stored in the shadow dx at byte offset 12, etc.</p><p>This has the nice property that the storage for the derivative, including all intermediate computations, is the same as that of the primal (ignoring caching requirements for reverse mode).</p><p>It also means that any arbitrary data structure can be differentiated with respect to, and we don’t have any special handling required to register every data structure one could create.</p><p>This representation does have some caveats (e.g. see Identical types in <code>Duplicated</code> above).</p><p>Sparse data structures are often represented with say a Vector{Float64} that holds the actual elements, and a Vector{Int} that specifies the index n the backing array that corresponds to the true location in the overall vector.</p><p>We have no explicit special cases for sparse Data structures, so the layout semantics mentioned above is indeed what Enzyme uses.</p><p>Thus the derivative of a sparse array is to have a second backing array of the same size, and another Vector{Int} (of the same offsets).</p><p>As a concrete example, suppose we have the following: <code>x = { 3 : 2.7, 10 : 3.14 }</code>. In other words, a sparse data structure with two elements, one at index 3, another at index 10. This could be represented with the backing array being <code>[2.7, 3.14]</code> and the index array being <code>[3, 10]</code>.</p><p>A correctly zero-initialized shadow data structure would be to have a backing array of size 2 with zero’s, and an index array again being <code>[3, 10]</code>.</p><p>In this form the second element of the derivative backing array is used to store/represent the derivative of the second element of the original backing array, in other words the derivative at index 10.</p><p>Like mentioned above, a caveat here is that this correctly zero’d initializer is not the default produced by <code>sparse([0.0])</code> as this drops the zero elements from the backing array. Enzyme.make<em>zero recursively goes through your data structure to generate the shadows of the correct structure (and in this case would make a new backing array of appropriate size). The `make</em>zero` function is not special cased to sparsity, but just comes out as a result.</p><p>Internally, when differentiating a function this is the type of data structure that Enzyme builds and uses to represent variables. However, at the Julia level that there’s a bit of a sharp edge.</p><p>Consider a function <code>f(A(x))</code> where <code>x</code> is a scalar or dense input, <code>A(x)</code> returns a sparse array, and <code>f(A(x))</code> returns a scalar loss. </p><p>The derivative that Enzyme creates for <code>A(x)</code> would create both the backing/index arrays for the original result A, as well as the equal sized backing/index arrays for the derivative.</p><p>For any program which generates sparse data structures internally, like the total program <code>f(A(x))</code>, this will always give you the answer you expect. Moreover, the memory requirements of the derivative will be the same as the primal (other AD tools will blow up the memory usage and construct dense derivatives where the primal was sparse).</p><p>The added caveat, however, comes when you differentiate a top level function that has a sparse array input. For example, consider the sparse <code>sum</code> function which adds up all elements. While in one definition, this function represents summing up all elements of the virtual sparse array (including the zero&#39;s which are never materialized), in a more literal sense this <code>sum</code> function will only add elements 3 and 10 of the input sparse array – the only two nonzero elements – or equivalently the sum of the whole backing array. Correspondingly Enzyme will update the sparse shadow data structure to mark both elements 3 and 10 as having a derivative of 1 (or more literally set all the elements of the backing array to derivative 1). These are the only variables that Enzyme needs to update, since they are the only variables read (and thus the only ones which have a non-zero derivative). Thus any function which may call this method and compose via the chain rule will only ever read the derivative of these two elements. This is why this memory-safe representation composes within Enzyme, though may produce counter-intuitive reuslts at the top level.</p><p>If the name we gave to this data structure wasn’t &quot;SparseArray&quot; but instead &quot;MyStruct&quot; this is precisely the answer we would have desired. However, since the sparse array printer prints zeros for elements outside of the sparse backing array, this isn’t what one would expect. Making a nicer user conversion from Enzyme’s form of differential data structures, to the more natural &quot;Julia&quot; form where there is a semantic mismatch between what Julia intends a data structure to mean by name, and what is being discussed <a href="https://github.com/EnzymeAD/Enzyme.jl/issues/1334">here</a>.</p><p>The benefit of this representation is that : (1) all of our rules compose correctly (you get the correct answer for <code>f(A(x)</code>), (2) without the need to special case any sparse code, and (3) with the same memory/performance expectations as the original code.</p><h2 id="Activity-of-temporary-storage"><a class="docs-heading-anchor" href="#Activity-of-temporary-storage">Activity of temporary storage</a><a id="Activity-of-temporary-storage-1"></a><a class="docs-heading-anchor-permalink" href="#Activity-of-temporary-storage" title="Permalink"></a></h2><p>If you pass in any temporary storage which may be involved in an active computation to a function you want to differentiate, you must also pass in a duplicated temporary storage for use in computing the derivatives. For example, consider the following function which uses a temporary buffer to compute the result.</p><pre><code class="language-julia hljs">function f(x, tmp, k, n)
    tmp[1] = 1.0
    for i in 1:n
        tmp[k] *= x
    end
    tmp[1]
end

# output

f (generic function with 1 method)</code></pre><p>Marking the argument for <code>tmp</code> as Const (aka non-differentiable) means that Enzyme believes that all variables loaded from or stored into <code>tmp</code> must also be non-differentiable, since all values inside a non-differentiable variable must also by definition be non-differentiable.</p><pre><code class="language-julia hljs">Enzyme.autodiff(Reverse, f, Active(1.2), Const(Vector{Float64}(undef, 1)), Const(1), Const(5))  # Incorrect

# output

((0.0, nothing, nothing, nothing),)</code></pre><p>Passing in a duplicated (e.g. differentiable) variable for <code>tmp</code> now leads to the correct answer.</p><pre><code class="language-julia hljs">Enzyme.autodiff(Reverse, f, Active(1.2), Duplicated(Vector{Float64}(undef, 1), zeros(1)), Const(1), Const(5))  # Correct (returns 10.367999999999999 == 1.2^4 * 5)

# output

((10.367999999999999, nothing, nothing, nothing),)</code></pre><h2 id="Runtime-Activity"><a class="docs-heading-anchor" href="#Runtime-Activity">Runtime Activity</a><a id="Runtime-Activity-1"></a><a class="docs-heading-anchor-permalink" href="#Runtime-Activity" title="Permalink"></a></h2><p>When computing the derivative of mutable variables, Enzyme also needs additional temporary storage space for the corresponding derivative variables. If an argument <code>tmp</code> is marked as Const, Enzyme does not have any temporary storage space for the derivatives!</p><p>Enzyme will error when they detect these latter types of situations, which we will refer to as <code>activity unstable</code>. This term is chosen to mirror the Julia notion of type-unstable code (e.g. where a type is not known at compile time). If an expression is activity unstable, it could either be constant, or active, depending on data not known at compile time. For example, consider the following:</p><pre><code class="language-julia hljs">function g(cond, active_var, constant_var)
  if cond
    return active_var
  else
    return constant_var
end

Enzyme.autodiff(Forward, g, Const(condition), Duplicated(x, dx), Const(y))</code></pre><p>The returned value here could either by constant or duplicated, depending on the runtime-defined value of <code>cond</code>. If <code>cond</code> is true, Enzyme simply returns the shadow of <code>active_var</code> as the derivative. However, if <code>cond</code> is false, there is no derivative shadow for <code>constant_var</code> and Enzyme will throw a <code>EnzymeRuntimeActivityError</code> error. For some simple types, e.g. a float Enzyme can circumvent this issue, for example by returning the float 0. Similarly, for some types like the Symbol type, which are never differentiable, such a shadow value will never be used, and Enzyme can return the original &quot;primal&quot; value as its derivative.  However, for arbitrary data structures, Enzyme presently has no generic mechanism to resolve this.</p><p>For example consider a third function:</p><pre><code class="language-julia hljs">function h(cond, active_var, constant_var)
  return [g(cond, active_var, constant_var), g(cond, active_var, constant_var)]
end

Enzyme.autodiff(Forward, h, Const(condition), Duplicated(x, dx), Const(y))</code></pre><p>Enzyme provides a nice utility <code>Enzyme.make_zero</code> which takes a data structure and constructs a deepcopy of the data structure with all of the floats set to zero and non-differentiable types like Symbols set to their primal value. If Enzyme gets into such a &quot;Mismatched activity&quot; situation where it needs to return a differentiable data structure from a constant variable, it could try to resolve this situation by constructing a new shadow data structure, such as with <code>Enzyme.make_zero</code>. However, this still can lead to incorrect results. In the case of <code>h</code> above, suppose that <code>active_var</code> and <code>consant_var</code> are both arrays, which are mutable (aka in-place) data types. This means that the return of <code>h</code> is going to either be <code>result = [active_var, active_var]</code> or <code>result = [constant_var, constant_var]</code>.  Thus an update to <code>result[1][1]</code> would also change <code>result[2][1]</code> since <code>result[1]</code> and <code>result[2]</code> are the same array. </p><p>If one created a new zero&#39;d copy of each return from <code>g</code>, this would mean that the derivative <code>dresult</code> would have one copy made for the first element, and a second copy made for the second element. This could lead to incorrect results, and is unfortunately not a general resolution. However, for non-mutable variables (e.g. like floats) or non-differrentiable types (e.g. like Symbols) this problem can never arise.</p><p>Instead, Enzyme has a special mode known as &quot;Runtime Activity&quot; which can handle these types of situations. It can come with a minor performance reduction, and is therefore off by default. It can be enabled with by setting runtime activity to true in a desired differentiation mode.</p><p>The way Enzyme&#39;s runtime activity resolves this issue is to return the original primal variable as the derivative whenever it needs to denote the fact that a variable is a constant. As this issue can only arise with mutable variables, they must be represented in memory via a pointer. All addtional loads and stores will now be modified to first check if the primal pointer is the same as the shadow pointer, and if so, treat it as a constant. Note that this check is not saying that the same arrays contain the same values, but rather the same backing memory represents both the primal and the shadow (e.g. <code>a === b</code> or equivalently <code>pointer(a) == pointer(b)</code>). </p><p>Enabling runtime activity does therefore, come with a sharp edge, which is that if the computed derivative of a function is mutable, one must also check to see if the primal and shadow represent the same pointer, and if so the true derivative of the function is actually zero.</p><p>Generally, the preferred solution to these type of activity unstable codes should be to make your variables all activity-stable (e.g. always containing differentiable memory or always containing non-differentiable memory). However, with care, Enzyme does support &quot;Runtime Activity&quot; as a way to differentiate these programs without having to modify your code. One can enable runtime activity for your code by changing the mode, such as</p><pre><code class="language-julia hljs">Enzyme.autodiff(set_runtime_activity(Forward), h, Const(condition), Duplicated(x, dx), Const(y))</code></pre><h2 id="Mixed-activity"><a class="docs-heading-anchor" href="#Mixed-activity">Mixed activity</a><a id="Mixed-activity-1"></a><a class="docs-heading-anchor-permalink" href="#Mixed-activity" title="Permalink"></a></h2><p>Sometimes in Reverse mode (but not forward mode), you may see an error <code>Type T has mixed internal activity types</code> for some type. This error arises when a variable in a computation cannot be fully represented as either a Duplicated or Active variable.</p><p>Active variables are used for immutable variables (like <code>Float64</code>), whereas Duplicated variables are used for mutable variables (like <code>Vector{Float64}</code>). Speciically, since Active variables are immutable, functions with Active inputs will return the adjoint of that variable. In contrast Duplicated variables will have their derivatives <code>+=</code>&#39;d in place.</p><p>This error indicates that you have a type, like <code>Tuple{Float, Vector{Float64}}</code> that has immutable components and mutable components. Therefore neither Active nor Duplicated can be used for this type.</p><p>Internally, by virtue of working at the LLVM level, most Julia types are represented as pointers, and this issue does not tend to arise within code fully differentiated by Enzyme internally. However, when a program needs to interact with Julia API&#39;s (e.g. as arguments to a custom rule, a type unstable call, or the outermost function being differentiated), Enzyme must adhere to Julia&#39;s notion of immutability and will throw this error rather than risk an incorrect result.</p><p>For example, consider the following code, which has a type unstable call to <code>myfirst</code>, passing in a mixed type <code>Tuple{Float64, Vector{Float64}}</code>.</p><pre><code class="language-julia hljs">@noinline function myfirst(tup::T) where T
    return tup[1]
end

function f(x::Float64)
    vec = [x]
    tup = (x, vec)
    Base.inferencebarrier(myfirst)(tup)::Float64
end

Enzyme.autodiff(Reverse, f, Active, Active(3.1))</code></pre><p>When this situation arises, it is often easiest to resolve it by adding a level of indirection to ensure the entire variable is mutable. For example, one could enclose this variable in a reference, such as <code>Ref{Tuple{Float, Vector{Float64}}}</code>, like as follows.</p><pre><code class="language-julia hljs">@noinline function myfirst_ref(tup_ref::T) where T
    tup = tup_ref[]
    return tup[1]
end

function f2(x::Float64)
    vec = [x]
    tup = (x, vec)
    tup_ref = Ref(tup)
    Base.inferencebarrier(myfirst_ref)(tup_ref)::Float64
end

Enzyme.autodiff(Reverse, f2, Active, Active(3.1))</code></pre><h2 id="Complex-numbers"><a class="docs-heading-anchor" href="#Complex-numbers">Complex numbers</a><a id="Complex-numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Complex-numbers" title="Permalink"></a></h2><p>Differentiation of a function which returns a complex number is ambiguous, because there are several different gradients which may be desired. Rather than assume a specific of these conventions and potentially result in user error when the resulting derivative is not the desired one, Enzyme forces users to specify the desired convention by returning a real number instead.</p><p>Consider the function <code>f(z) = z*z</code>. If we were to differentiate this and have real inputs and outputs, the derivative <code>f&#39;(z)</code> would be unambiguously <code>2*z</code>. However, consider breaking down a complex number down into real and imaginary parts. Suppose now we were to call <code>f</code> with the explicit real and imaginary components, <code>z = x + i y</code>. This means that <code>f</code> is a function that takes an input of two values and returns two values <code>f(x, y) = u(x, y) + i v(x, y)</code>. In the case of <code>z*z</code> this means that <code>u(x,y) = x*x-y*y</code> and <code>v(x,y) = 2*x*y</code>.</p><p>If we were to look at all first-order derivatives in total, we would end up with a 2x2 matrix (i.e. Jacobian), the derivative of each output wrt each input. Let&#39;s try to compute this, first by hand, then with Enzyme.</p><pre><code class="nohighlight hljs">grad u(x, y) = [d/dx u, d/dy u] = [d/dx x*x-y*y, d/dy x*x-y*y] = [2*x, -2*y];
grad v(x, y) = [d/dx v, d/dy v] = [d/dx 2*x*y, d/dy 2*x*y] = [2*y, 2*x];</code></pre><p>Reverse mode differentiation computes the derivative of all inputs with respect to a single output by propagating the derivative of the return to its inputs. Here, we can explicitly differentiate with respect to the real and imaginary results, respectively, to find this matrix.</p><pre><code class="language-julia hljs">f(z) = z * z

# a fixed input to use for testing
z = 3.1 + 2.7im

grad_u = Enzyme.autodiff(Reverse, z-&gt;real(f(z)), Active, Active(z))[1][1]
grad_v = Enzyme.autodiff(Reverse, z-&gt;imag(f(z)), Active, Active(z))[1][1]

(grad_u, grad_v)
# output
(6.2 - 5.4im, 5.4 + 6.2im)</code></pre><p>This is somewhat inefficient, since we need to call the forward pass twice, once for the real part, once for the imaginary. We can solve this using batched derivatives in Enzyme, which computes several derivatives for the same function all in one go. To make it work, we&#39;re going to need to use split mode, which allows us to provide a custom derivative return value.</p><pre><code class="language-julia hljs">fwd, rev = Enzyme.autodiff_thunk(ReverseSplitNoPrimal, Const{typeof(f)}, Active, Active{ComplexF64})

# Compute the reverse pass seeded with a differntial return of 1.0 + 0.0im
grad_u = rev(Const(f), Active(z), 1.0 + 0.0im, fwd(Const(f), Active(z))[1])[1][1]
# Compute the reverse pass seeded with a differntial return of 0.0 + 1.0im
grad_v = rev(Const(f), Active(z), 0.0 + 1.0im, fwd(Const(f), Active(z))[1])[1][1]

(grad_u, grad_v)

# output
(6.2 - 5.4im, 5.4 + 6.2im)</code></pre><p>Now let&#39;s make this batched</p><pre><code class="language-julia hljs">fwd, rev = Enzyme.autodiff_thunk(ReverseSplitWidth(ReverseSplitNoPrimal, Val(2)), Const{typeof(f)}, Active, Active{ComplexF64})

# Compute the reverse pass seeded with a differential return of 1.0 + 0.0im and 0.0 + 1.0im in one go!
rev(Const(f), Active(z), (1.0 + 0.0im, 0.0 + 1.0im), fwd(Const(f), Active(z))[1])[1][1]

# output
(6.2 - 5.4im, 5.4 + 6.2im)</code></pre><p>In contrast, Forward mode differentiation computes the derivative of all outputs with respect to a single input by providing a differential input. Thus we need to seed the shadow input with either 1.0 or 1.0im, respectively. This will compute the transpose of the matrix we found earlier.</p><pre><code class="nohighlight hljs">d/dx f(x, y) = d/dx [u(x,y), v(x,y)] = d/dx [x*x-y*y, 2*x*y] = [ 2*x, 2*y];
d/dy f(x, y) = d/dy [u(x,y), v(x,y)] = d/dy [x*x-y*y, 2*x*y] = [-2*y, 2*x];</code></pre><pre><code class="language-julia hljs">d_dx = Enzyme.autodiff(Forward, f, Duplicated(z, 1.0+0.0im))[1]
d_dy = Enzyme.autodiff(Forward, f, Duplicated(z, 0.0+1.0im))[1]

(d_dx, d_dy)

# output
(6.2 + 5.4im, -5.4 + 6.2im)</code></pre><p>Again, we can go ahead and batch this.</p><pre><code class="language-julia hljs">Enzyme.autodiff(Forward, f, BatchDuplicated(z, (1.0+0.0im, 0.0+1.0im)))[1]

# output
(var&quot;1&quot; = 6.2 + 5.4im, var&quot;2&quot; = -5.4 + 6.2im)</code></pre><p>Taking Jacobians with respect to the real and imaginary results is fine, but for a complex scalar function it would be really nice to have a single complex derivative. More concretely, in this case when differentiating <code>z*z</code>, it would be nice to simply return <code>2*z</code>. However, there are four independent variables in the 2x2 jacobian, but only two in a complex number. </p><p>Complex differentiation is often viewed in the lens of directional derivatives. For example, what is the derivative of the function as the real input increases, or as the imaginary input increases. Consider the derivative along the real axis, <span>$\texttt{lim}_{\Delta x \rightarrow 0} \frac{f(x+\Delta x, y)-f(x, y)}{\Delta x}$</span>. This simplifies to <span>$\texttt{lim}_{\Delta x \rightarrow 0} \frac{u(x+\Delta x, y)-u(x, y) + i \left[ v(x+\Delta x, y)-v(x, y)\right]}{\Delta x} = \frac{\partial}{\partial x} u(x,y) + i\frac{\partial}{\partial x} v(x,y)$</span>. This is exactly what we computed by seeding forward mode with a shadow of <code>1.0 + 0.0im</code>.</p><p>For completeness, we can also consider the derivative along the imaginary axis  <span>$\texttt{lim}_{\Delta y \rightarrow 0} \frac{f(x, y+\Delta y)-f(x, y)}{i\Delta y}$</span>. Here this simplifies to <span>$\texttt{lim}_{u(x, y+\Delta y)-u(x, y) + i \left[ v(x, y+\Delta y)-v(x, y)\right]}{i\Delta y} = -i\frac{\partial}{\partial y} u(x,y) + \frac{\partial}{\partial y} v(x,y)$</span>. Except for the <span>$i$</span> in the denominator of the limit, this is the same as the result of Forward mode, when seeding x with a shadow of <code>0.0 + 1.0im</code>. We can thus compute the derivative along the real axis by multiplying our second Forward mode call by <code>-im</code>.</p><pre><code class="language-julia hljs">d_real = Enzyme.autodiff(Forward, f, Duplicated(z, 1.0+0.0im))[1]
d_im   = -im * Enzyme.autodiff(Forward, f, Duplicated(z, 0.0+1.0im))[1]

(d_real, d_im)

# output
(6.2 + 5.4im, 6.2 + 5.4im)</code></pre><p>Interestingly, the derivative of <code>z*z</code> is the same when computed in either axis. That is because this function is part of a special class of functions that are invariant to the input direction, called holomorphic. </p><p>Thus, for holomorphic functions, we can simply seed Forward-mode AD with a shadow of one for whatever input we are differenitating. This is nice since seeding the shadow with an input of one is exactly what we&#39;d do for real-valued funtions as well.</p><p>Reverse-mode AD, however, is more tricky. This is because holomorphic functions are invariant to the direction of differentiation (aka the derivative inputs), not the direction of the differential return.</p><p>However, if a function is holomorphic, the two derivative functions we computed above must be the same. As a result, <span>$\frac{\partial}{\partial x} u = \frac{\partial}{\partial y} v$</span> and <span>$\frac{\partial}{\partial y} u = -\frac{\partial}{\partial x} v$</span>. </p><p>We saw earlier, that performing reverse-mode AD with a return seed of <code>1.0 + 0.0im</code> yielded <code>[d/dx u, d/dy u]</code>. Thus, for a holomorphic function, a real-seeded Reverse-mode AD computes <code>[d/dx u, -d/dx v]</code>, which is the complex conjugate of the derivative.</p><pre><code class="language-julia hljs">conj(grad_u)

# output

6.2 + 5.4im</code></pre><p>In the case of a scalar-input scalar-output function, that&#39;s sufficient. However, most of the time one uses reverse mode, it involves either several inputs or outputs, perhaps via memory. This case requires additional handling to properly sum all the partial derivatives from the use of each input and apply the conjugate operator at only the ones relevant to the differential return.</p><p>For simplicity, Enzyme provides a helper utlity <code>ReverseHolomorphic</code> which performs Reverse mode properly here, assuming that the function is indeed holomorphic and thus has a well-defined single derivative.</p><pre><code class="language-julia hljs">Enzyme.autodiff(ReverseHolomorphic, f, Active, Active(z))[1][1]

# output

6.2 + 5.4im</code></pre><p>For even non-holomorphic functions, complex analysis allows us to define <span>$\frac{\partial}{\partial z} = \frac{1}{2}\left(\frac{\partial}{\partial x} - i \frac{\partial}{\partial y} \right)$</span>. For non-holomorphic functions, this allows us to compute <code>d/dz</code>.  Let&#39;s consider <code>myabs2(z) = z * conj(z)</code>. We can compute the derivative wrt z of this in Forward mode as follows, which as one would expect results in a result of <code>conj(z)</code>:</p><pre><code class="language-julia hljs">myabs2(z) = z * conj(z)

dabs2_dx, dabs2_dy = Enzyme.autodiff(Forward, myabs2, BatchDuplicated(z, (1.0 + 0.0im, 0.0 + 1.0im)))[1]
(dabs2_dx - im * dabs2_dy) / 2

# output

3.1 - 2.7im</code></pre><p>Similarly, we can compute <code>d/d conj(z) = d/dx + i d/dy</code>.</p><pre><code class="language-julia hljs">(dabs2_dx + im * dabs2_dy) / 2

# output

3.1 + 2.7im</code></pre><p>Computing this in Reverse mode is more tricky. Let&#39;s expand <code>f</code> in terms of <code>u</code> and <code>v</code>. <span>$\frac{\partial}{\partial z} f = \frac12 \left( [u_x + i v_x] - i [u_y + i v_y] \right) = \frac12 \left( [u_x + v_y] + i [v_x - u_y] \right)$</span>. Thus <code>d/dz = (conj(grad_u) + im * conj(grad_v))/2</code>.</p><pre><code class="language-julia hljs">abs2_fwd, abs2_rev = Enzyme.autodiff_thunk(ReverseSplitWidth(ReverseSplitNoPrimal, Val(2)), Const{typeof(myabs2)}, Active, Active{ComplexF64})

# Compute the reverse pass seeded with a differential return of 1.0 + 0.0im and 0.0 + 1.0im in one go!
gradabs2_u, gradabs2_v = abs2_rev(Const(myabs2), Active(z), (1.0 + 0.0im, 0.0 + 1.0im), abs2_fwd(Const(myabs2), Active(z))[1])[1][1]

(conj(gradabs2_u) + im * conj(gradabs2_v)) / 2

# output

3.1 - 2.7im</code></pre><p>For <code>d/d conj(z)</code>, <span>$\frac12 \left( [u_x + i v_x] + i [u_y + i v_y] \right) = \frac12 \left( [u_x - v_y] + i [v_x + u_y] \right)$</span>. Thus <code>d/d conj(z) = (grad_u + im * grad_v)/2</code>.</p><pre><code class="language-julia hljs">(gradabs2_u + im * gradabs2_v) / 2

# output

3.1 + 2.7im</code></pre><p>Note: when writing rules for complex scalar functions, in reverse mode one needs to conjugate the differential return, and similarly the true result will be the conjugate of that value (in essence you can think of reverse-mode AD as working in the conjugate space).</p><h2 id="What-types-are-differentiable?"><a class="docs-heading-anchor" href="#What-types-are-differentiable?">What types are differentiable?</a><a id="What-types-are-differentiable?-1"></a><a class="docs-heading-anchor-permalink" href="#What-types-are-differentiable?" title="Permalink"></a></h2><p>Enzyme tracks differentiable dataflow through values. Specifically Enzyme tracks differentiable data in base types like Float32, Float64, Float16, BFloat16, etc.</p><p>As a simple example:</p><pre><code class="language-julia hljs">f(x) = x * x
Enzyme.autodiff(Forward, f, Duplicated(3.0, 1.0))

# output

(6.0,)</code></pre><p>Enzyme also tracks differentiable data in any types containing these base types (e.g. floats). For example, consider a struct or array containing floats.</p><pre><code class="language-julia hljs">struct Pair
    lhs::Float64
    rhs::Float64
end
f_pair(x) = x.lhs * x.rhs
Enzyme.autodiff(Forward, f_pair, Duplicated(Pair(3.0, 2.0), Pair(1.0, 0.0)))

# output

(2.0,)</code></pre><pre><code class="language-julia hljs">Enzyme.autodiff(Forward, sum, Duplicated([1.0, 2.0, 3.0], [5.0, 0.0, 100.0]))


# output

(105.0,)</code></pre><p>A differentiable data structure can be arbitrarily complex, such as a linked list.</p><pre><code class="language-julia hljs">
struct LList
    prev::Union{Nothing, LList}
    value::Float64
end

function make_list(x::Vector)
   result = nothing
   for value in reverse(x)
      result = LList(result, value)
   end
   return result
end

function list_sum(list::Union{Nothing, LList})
   result = 0.0
   while list != nothing
     result += list.value
     list = list.prev
   end
   return result
end

list = make_list([1.0, 2.0, 3.0])
dlist = make_list([5.0, 0.0, 100.0])

Enzyme.autodiff(Forward, list_sum, Duplicated(list, dlist))

# output

(105.0,)</code></pre><p>Presently Enzyme only considers floats as base types. As a result, Enzyme does not support differentiating data contained in Ints, Strings, or Vals. If it is desirable for Enzyme to add a base type, please open an issue.</p><pre><code class="language-julia hljs">f_int(x) = x * x
Enzyme.autodiff(Forward, f_int, Duplicated, Duplicated(3, 1))

# output

ERROR: Return type `Int64` not marked Const, but type is guaranteed to be constant</code></pre><pre><code class="language-julia hljs">f_str(x) = parse(Float64, x) * parse(Float64, x)

autodiff(Forward, f_str, Duplicated(&quot;1.0&quot;, &quot;1.0&quot;))

# output

(0.0,)</code></pre><pre><code class="language-julia hljs">f_val(::Val{x}) where x = x * x

autodiff(Forward, f_val, Duplicated(Val(1.0), Val(1.0)))

# output

ERROR: Type of ghost or constant type Duplicated{Val{1.0}} is marked as differentiable.</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../generated/custom_rule/">« Custom rules</a><a class="docs-footer-nextpage" href="../api/">API reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Tuesday 24 September 2024 19:03">Tuesday 24 September 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
