<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Enzyme.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://enzyme.mit.edu/julia/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><script src="https://plausible.io/js/plausible.js" data-domain="enzyme.mit.edu" defer></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.svg" alt="Enzyme.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>Enzyme.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Getting-started"><span>Getting started</span></a></li><li><a class="tocitem" href="#Caveats-/-Known-issues"><span>Caveats / Known-issues</span></a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="generated/box/">Box model</a></li><li><a class="tocitem" href="generated/autodiff/">AutoDiff API</a></li><li><a class="tocitem" href="generated/custom_rule/">Custom rules</a></li></ul></li><li><a class="tocitem" href="api/">API</a></li><li><a class="tocitem" href="pullbacks/">Implementing pullbacks</a></li><li><a class="tocitem" href="dev_docs/">For developers</a></li><li><a class="tocitem" href="internal_api/">Internal API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/EnzymeAD/Enzyme.jl/blob/main/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Enzyme"><a class="docs-heading-anchor" href="#Enzyme">Enzyme</a><a id="Enzyme-1"></a><a class="docs-heading-anchor-permalink" href="#Enzyme" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a>, the Julia bindings for <a href="https://github.com/EnzymeAD/enzyme">Enzyme</a>.</p><p>Enzyme performs automatic differentiation (AD) of statically analyzable LLVM. It is highly-efficient and its ability to perform AD on optimized code allows Enzyme to meet or exceed the performance of state-of-the-art AD tools.</p><p>Enzyme.jl can be installed in the usual way Julia packages are installed:</p><pre><code class="nohighlight hljs">] add Enzyme</code></pre><p>The Enzyme binary dependencies will be installed automatically via Julia&#39;s binary artifact system.</p><p>The Enzyme.jl API revolves around the function <a href="api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{RABI}, Tuple{ForwardMode{RABI}, FA, Type{A}, Vararg{Any}}} where {RABI&lt;:EnzymeCore.ABI, FA&lt;:Annotation, A&lt;:Annotation}"><code>autodiff</code></a>. For some common operations, Enzyme additionally wraps <a href="api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{RABI}, Tuple{ForwardMode{RABI}, FA, Type{A}, Vararg{Any}}} where {RABI&lt;:EnzymeCore.ABI, FA&lt;:Annotation, A&lt;:Annotation}"><code>autodiff</code></a> in several convenience functions; e.g., <a href="api/#Enzyme.gradient-Tuple{ForwardMode, Any, Any}"><code>gradient</code></a> and <a href="api/#Enzyme.jacobian-Tuple{ForwardMode, Any, Any}"><code>jacobian</code></a>.</p><p>The tutorial below covers the basic usage of these functions. For a complete overview of Enzyme&#39;s functionality, see the <a href="api/#API">API</a> documentation. Also see <a href="pullbacks/#Implementing-pullbacks">Implementing pullbacks</a> on how to implement back-propagation for functions with non-scalar results.</p><h2 id="Getting-started"><a class="docs-heading-anchor" href="#Getting-started">Getting started</a><a id="Getting-started-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-started" title="Permalink"></a></h2><pre><code class="language-julia-repl hljs">julia&gt; rosenbrock(x, y) = (1.0 - x)^2 + 100.0 * (y - x^2)^2
rosenbrock (generic function with 1 method)

julia&gt; rosenbrock_inp(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
rosenbrock_inp (generic function with 1 method)</code></pre><h3 id="Reverse-mode"><a class="docs-heading-anchor" href="#Reverse-mode">Reverse mode</a><a id="Reverse-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-mode" title="Permalink"></a></h3><p>The return value of reverse mode <a href="api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{RABI}, Tuple{ForwardMode{RABI}, FA, Type{A}, Vararg{Any}}} where {RABI&lt;:EnzymeCore.ABI, FA&lt;:Annotation, A&lt;:Annotation}"><code>autodiff</code></a> is a tuple that contains as a first value the derivative value of the active inputs and optionally the primal return value.</p><pre><code class="language-julia-repl hljs">julia&gt; autodiff(Reverse, rosenbrock, Active, Active(1.0), Active(2.0))
((-400.0, 200.0),)

julia&gt; autodiff(ReverseWithPrimal, rosenbrock, Active, Active(1.0), Active(2.0))
((-400.0, 200.0), 100.0)</code></pre><pre><code class="language-julia-repl hljs">julia&gt; x = [1.0, 2.0]
2-element Vector{Float64}:
 1.0
 2.0

julia&gt; dx = [0.0, 0.0]
2-element Vector{Float64}:
 0.0
 0.0

julia&gt; autodiff(Reverse, rosenbrock_inp, Active, Duplicated(x, dx))
((nothing,),)

julia&gt; dx
2-element Vector{Float64}:
 -400.0
  200.0</code></pre><p>Both the inplace and &quot;normal&quot; variant return the gradient. The difference is that with <a href="api/#EnzymeCore.Active"><code>Active</code></a> the gradient is returned and with <a href="api/#EnzymeCore.Duplicated"><code>Duplicated</code></a> the gradient is accumulated in place.</p><h3 id="Forward-mode"><a class="docs-heading-anchor" href="#Forward-mode">Forward mode</a><a id="Forward-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-mode" title="Permalink"></a></h3><p>The return value of forward mode with a <code>Duplicated</code> return is a tuple containing as the first value the primal return value and as the second value the derivative.</p><p>In forward mode <code>Duplicated(x, 0.0)</code> is equivalent to <code>Const(x)</code>, except that we can perform more optimizations for <code>Const</code>.</p><pre><code class="language-julia-repl hljs">julia&gt; autodiff(Forward, rosenbrock, Duplicated, Const(1.0), Duplicated(3.0, 1.0))
(400.0, 400.0)

julia&gt; autodiff(Forward, rosenbrock, Duplicated, Duplicated(1.0, 1.0), Const(3.0))
(400.0, -800.0)</code></pre><p>Of note, when we seed both arguments at once the tangent return is the sum of both.</p><pre><code class="language-julia-repl hljs">julia&gt; autodiff(Forward, rosenbrock, Duplicated, Duplicated(1.0, 1.0), Duplicated(3.0, 1.0))
(400.0, -400.0)</code></pre><p>We can also use forward mode with our inplace method.</p><pre><code class="language-julia-repl hljs">julia&gt; x = [1.0, 3.0]
2-element Vector{Float64}:
 1.0
 3.0

julia&gt; dx = [1.0, 1.0]
2-element Vector{Float64}:
 1.0
 1.0

julia&gt; autodiff(Forward, rosenbrock_inp, Duplicated, Duplicated(x, dx))
(400.0, -400.0)</code></pre><p>Note the seeding through <code>dx</code>.</p><h4 id="Vector-forward-mode"><a class="docs-heading-anchor" href="#Vector-forward-mode">Vector forward mode</a><a id="Vector-forward-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-forward-mode" title="Permalink"></a></h4><p>We can also use vector mode to calculate both derivatives at once.</p><pre><code class="language-julia-repl hljs">julia&gt; autodiff(Forward, rosenbrock, BatchDuplicated, BatchDuplicated(1.0, (1.0, 0.0)), BatchDuplicated(3.0, (0.0, 1.0)))
(400.0, (var&quot;1&quot; = -800.0, var&quot;2&quot; = 400.0))

julia&gt; x = [1.0, 3.0]
2-element Vector{Float64}:
 1.0
 3.0

julia&gt; dx_1 = [1.0, 0.0]; dx_2 = [0.0, 1.0];

julia&gt; autodiff(Forward, rosenbrock_inp, BatchDuplicated, BatchDuplicated(x, (dx_1, dx_2)))
(400.0, (var&quot;1&quot; = -800.0, var&quot;2&quot; = 400.0))</code></pre><h3 id="Convenience-functions"><a class="docs-heading-anchor" href="#Convenience-functions">Convenience functions</a><a id="Convenience-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-functions" title="Permalink"></a></h3><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>While the convenience functions discussed below use <a href="api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{RABI}, Tuple{ForwardMode{RABI}, FA, Type{A}, Vararg{Any}}} where {RABI&lt;:EnzymeCore.ABI, FA&lt;:Annotation, A&lt;:Annotation}"><code>autodiff</code></a> internally, they are generally more limited in their functionality. Beyond that, these convenience functions may also come with performance penalties; especially if one makes a closure of a multi-argument function instead of calling the appropriate multi-argument <a href="api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{RABI}, Tuple{ForwardMode{RABI}, FA, Type{A}, Vararg{Any}}} where {RABI&lt;:EnzymeCore.ABI, FA&lt;:Annotation, A&lt;:Annotation}"><code>autodiff</code></a> function directly.</p></div></div><p>Key convenience functions for common derivative computations are <a href="api/#Enzyme.gradient-Tuple{ForwardMode, Any, Any}"><code>gradient</code></a> (and its inplace variant <a href="api/#Enzyme.gradient!-Tuple{ReverseMode, Any, Any, Any}"><code>gradient!</code></a>) and <a href="api/#Enzyme.jacobian-Tuple{ForwardMode, Any, Any}"><code>jacobian</code></a>. Like <a href="api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{RABI}, Tuple{ForwardMode{RABI}, FA, Type{A}, Vararg{Any}}} where {RABI&lt;:EnzymeCore.ABI, FA&lt;:Annotation, A&lt;:Annotation}"><code>autodiff</code></a>, the mode (forward or reverse) is determined by the first argument.</p><p>The functions <a href="api/#Enzyme.gradient-Tuple{ForwardMode, Any, Any}"><code>gradient</code></a> and <a href="api/#Enzyme.gradient!-Tuple{ReverseMode, Any, Any, Any}"><code>gradient!</code></a> compute the gradient of function with vector input and scalar return.</p><pre><code class="language-julia-repl hljs">julia&gt; gradient(Reverse, rosenbrock_inp, [1.0, 2.0])
2-element Vector{Float64}:
 -400.0
  200.0

julia&gt; # inplace variant
       dx = [0.0, 0.0];
       gradient!(Reverse, dx, rosenbrock_inp, [1.0, 2.0])
2-element Vector{Float64}:
 -400.0
  200.0

julia&gt; dx
2-element Vector{Float64}:
 -400.0
  200.0

julia&gt; gradient(Forward, rosenbrock_inp, [1.0, 2.0])
(-400.0, 200.0)

julia&gt; # in forward mode, we can also optionally pass a chunk size
       # to specify the number of derivatives computed simulateneously
       # using vector forward mode
       chunk_size = Val(2)
       gradient(Forward, rosenbrock_inp, [1.0, 2.0], chunk_size)
(-400.0, 200.0)</code></pre><p>The function <a href="api/#Enzyme.jacobian-Tuple{ForwardMode, Any, Any}"><code>jacobian</code></a> computes the Jacobian of a function vector input and vector return.</p><pre><code class="language-julia-repl hljs">julia&gt; foo(x) = [rosenbrock_inp(x), prod(x)];

julia&gt; output_size = Val(2) # here we have to provide the output size of `foo` since it cannot be statically inferred
       jacobian(Reverse, foo, [1.0, 2.0], output_size) 
2×2 Matrix{Float64}:
 -400.0  200.0
    2.0    1.0

julia&gt; chunk_size = Val(2) # By specifying the optional chunk size argument, we can use vector inverse mode to propogate derivatives of multiple outputs at once.
       jacobian(Reverse, foo, [1.0, 2.0], output_size, chunk_size)
2×2 Matrix{Float64}:
 -400.0  200.0
    2.0    1.0

julia&gt; jacobian(Forward, foo, [1.0, 2.0])
2×2 Matrix{Float64}:
 -400.0  200.0
    2.0    1.0

julia&gt; # Again, the optinal chunk size argument allows us to use vector forward mode
       jacobian(Forward, foo, [1.0, 2.0], chunk_size)
2×2 Matrix{Float64}:
 -400.0  200.0
    2.0    1.0</code></pre><h2 id="Caveats-/-Known-issues"><a class="docs-heading-anchor" href="#Caveats-/-Known-issues">Caveats / Known-issues</a><a id="Caveats-/-Known-issues-1"></a><a class="docs-heading-anchor-permalink" href="#Caveats-/-Known-issues" title="Permalink"></a></h2><h3 id="Activity-of-temporary-storage-/-Activity-Unstable-Code"><a class="docs-heading-anchor" href="#Activity-of-temporary-storage-/-Activity-Unstable-Code">Activity of temporary storage / Activity Unstable Code</a><a id="Activity-of-temporary-storage-/-Activity-Unstable-Code-1"></a><a class="docs-heading-anchor-permalink" href="#Activity-of-temporary-storage-/-Activity-Unstable-Code" title="Permalink"></a></h3><p>If you pass in any temporary storage which may be involved in an active computation to a function you want to differentiate, you must also pass in a duplicated temporary storage for use in computing the derivatives. For example, consider the following function which uses a temporary buffer to compute the result.</p><pre><code class="language-julia hljs">function f(x, tmp, k, n)
    tmp[1] = 1.0
    for i in 1:n
        tmp[k] *= x
    end
    tmp[1]
end

# output

f (generic function with 1 method)</code></pre><p>Marking the argument for <code>tmp</code> as Const (aka non-differentiable) means that Enzyme believes that all variables loaded from or stored into <code>tmp</code> must also be non-differentiable, since all values inside a non-differentiable variable must also by definition be non-differentiable.</p><pre><code class="language-julia hljs">Enzyme.autodiff(Reverse, f, Active(1.2), Const(Vector{Float64}(undef, 1)), Const(1), Const(5))  # Incorrect

# output

((0.0, nothing, nothing, nothing),)</code></pre><p>Passing in a dupliacted (e.g. differentiable) variable for <code>tmp</code> now leads to the correct answer.</p><pre><code class="language-julia hljs">Enzyme.autodiff(Reverse, f, Active(1.2), Duplicated(Vector{Float64}(undef, 1), Vector{Float64}(undef, 1)), Const(1), Const(5))  # Correct (returns 10.367999999999999 == 1.2^4 * 5)

# output

((10.367999999999999, nothing, nothing, nothing),)</code></pre><p>However, even if we ignore the semantic guarantee provided by marking <code>tmp</code> as constant, another issue arises. When computing the original function, intermediate computations (like in <code>f</code> above) can use <code>tmp</code> for temporary storage. When computing the derivative, Enzyme also needs additional temporary storage space for the corresponding derivative variables as well. If <code>tmp</code> is marked as Const, Enzyme does not have any temporary storage space for the derivatives!</p><p>Recent versions of Enzyme will attempt to error when they detect these latter types of situations, which we will refer to as <code>activity unstable</code>. This term is chosen to mirror the Julia notion of type-unstable code (e.g. where a type is not known at compile time). If an expression is activity unstable, it could either be constant, or active, depending on data not known at compile time. For example, consider the following:</p><pre><code class="language-julia hljs">function g(cond, active_var, constant_var)
  if cond
    return active_var
  else
    return constant_var
end

Enzyme.autodiff(Forward, g, Const(condition), Duplicated(x, dx), Const(y))</code></pre><p>The returned value here could either by constant or duplicated, depending on the runtime-defined value of <code>cond</code>. If <code>cond</code> is true, Enzyme simply returns the shadow of <code>active_var</code> as the derivative. However, if <code>cond</code> is false, there is no derivative shadow for <code>constant_var</code> and Enzyme will throw a &quot;Mismatched activity&quot; error. For some simple types, e.g. a float Enzyme can circumvent this issue, for example by returning the float 0. Similarly, for some types like the Symbol type, which are never differentiable, such a shadow value will never be used, and Enzyme can return the original &quot;primal&quot; value as its derivative.  However, for arbitrary data structures, Enzyme presently has no generic mechanism to resolve this.</p><p>For example consider a third function:</p><pre><code class="language-julia hljs">function h(cond, active_var, constant_var)
  return [g(cond, active_var, constant_var), g(cond, active_var, constant_var)]
end

Enzyme.autodiff(Forward, h, Const(condition), Duplicated(x, dx), Const(y))</code></pre><p>Enzyme provides a nice utility <code>Enzyme.make_zero</code> which takes a data structure and constructs a deepcopy of the data structure with all of the floats set to zero and non-differentiable types like Symbols set to their primal value. If Enzyme gets into such a &quot;Mismatched activity&quot; situation where it needs to return a differentiable data structure from a constant variable, it could try to resolve this situation by constructing a new shadow data structure, such as with <code>Enzyme.make_zero</code>. However, this still can lead to incorrect results. In the case of <code>h</code> above, suppose that <code>active_var</code> and <code>consant_var</code> are both arrays, which are mutable (aka in-place) data types. This means that the return of <code>h</code> is going to either be <code>result = [active_var, active_var]</code> or <code>result = [constant_var, constant_var]</code>.  Thus an update to <code>result[1][1]</code> would also change <code>result[2][1]</code> since <code>result[1]</code> and <code>result[2]</code> are the same array. </p><p>If one created a new zero&#39;d copy of each return from <code>g</code>, this would mean that the derivative <code>dresult</code> would have one copy made for the first element, and a second copy made for the second element. This could lead to incorrect results, and is unfortunately not a general resolution. However, for non-mutable variables (e.g. like floats) or non-differrentiable types (e.g. like Symbols) this problem can never arise.</p><p>Instead, Enzyme has a special mode known as &quot;Runtime Activity&quot; which can handle these types of situations. It can come with a minor performance reduction, and is therefore off by default. It can be enabled with <code>Enzyme.API.runtimeActivity!(true)</code> right after importing Enzyme for the first time. </p><p>The way Enzyme&#39;s runtime activity resolves this issue is to return the original primal variable as the derivative whenever it needs to denote the fact that a variable is a constant. As this issue can only arise with mutable variables, they must be represented in memory via a pointer. All addtional loads and stores will now be modified to first check if the primal pointer is the same as the shadow pointer, and if so, treat it as a constant. Note that this check is not saying that the same arrays contain the same values, but rather the same backing memory represents both the primal and the shadow (e.g. <code>a === b</code> or equivalently <code>pointer(a) == pointer(b)</code>). </p><p>Enabling runtime activity does therefore, come with a sharp edge, which is that if the computed derivative of a function is mutable, one must also check to see if the primal and shadow represent the same pointer, and if so the true derivative of the function is actually zero.</p><p>Generally, the preferred solution to these type of activity unstable codes should be to make your variables all activity-stable (e.g. always containing differentiable memory or always containing non-differentiable memory). However, with care, Enzyme does support &quot;Runtime Activity&quot; as a way to differentiate these programs without having to modify your code.</p><h3 id="CUDA.jl-support"><a class="docs-heading-anchor" href="#CUDA.jl-support">CUDA.jl support</a><a id="CUDA.jl-support-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA.jl-support" title="Permalink"></a></h3><p><a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> is only supported on Julia v1.7.0 and onwards. On v1.6, attempting to differentiate CUDA kernel functions will not use device overloads correctly and thus returns fundamentally wrong results.</p><h3 id="Sparse-Arrays"><a class="docs-heading-anchor" href="#Sparse-Arrays">Sparse Arrays</a><a id="Sparse-Arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Arrays" title="Permalink"></a></h3><p>At the moment there is limited support for sparse linear algebra operations. Sparse arrays may be used, but care must be taken because backing arrays drop zeros in Julia (unless told not to).</p><pre><code class="language-julia hljs">using SparseArrays
a = sparse([2.0])
da1 = sparse([0.0]) # Incorrect: SparseMatrixCSC drops explicit zeros
Enzyme.autodiff(Reverse, sum, Active, Duplicated(a, da1))
da1

# output

1-element SparseVector{Float64, Int64} with 0 stored entries</code></pre><pre><code class="language-julia hljs">da2 = sparsevec([1], [0.0]) # Correct: Prevent SparseMatrixCSC from dropping zeros
Enzyme.autodiff(Reverse, sum, Active, Duplicated(a, da2))
da2

# output

1-element SparseVector{Float64, Int64} with 1 stored entry:
  [1]  =  1.0</code></pre><p>Sometimes, determining how to perform this zeroing can be complicated. That is why Enzyme provides a helper function <code>Enzyme.make_zero</code> that does this automatically.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="generated/box/">Box model »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 5 March 2024 01:45">Tuesday 5 March 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
