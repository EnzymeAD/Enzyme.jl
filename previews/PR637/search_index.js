var documenterSearchIndex = {"docs":
[{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"EditURL = \"https://github.com/EnzymeAD/Enzyme.jl/blob/main/examples/autodiff.jl\"","category":"page"},{"location":"generated/autodiff/#AutoDiff-API","page":"AutoDiff API","title":"AutoDiff API","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The goal of this tutorial is to give users already familiar with automatic differentiation (AD) an overview of the Enzyme differentiation API for the following differentiation modes","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Reverse mode\nForward mode\nForward over reverse mode\nVector Forward over reverse mode","category":"page"},{"location":"generated/autodiff/#Defining-a-function","page":"AutoDiff API","title":"Defining a function","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme differentiates arbitrary multivariate vector functions as the most general case in automatic differentiation","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"f mathbbR^n rightarrow mathbbR^m y = f(x)","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"For simplicity we define a vector function with m=1. However, this tutorial can easily be applied to arbitrary m in mathbbN.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"using Enzyme\n\nfunction f(x::Array{Float64}, y::Array{Float64})\n    y[1] = x[1] * x[1] + x[2] * x[1]\n    return nothing\nend;\nnothing #hide","category":"page"},{"location":"generated/autodiff/#Reverse-mode","page":"AutoDiff API","title":"Reverse mode","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The reverse model in AD is defined as","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"beginaligned\ny = f(x) \nbarx = bary cdot nabla f(x)\nendaligned","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"bar denotes an adjoint variable. Note that executing an AD in reverse mode computes both y and the adjoint barx.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"x  = [2.0, 2.0]\nbx = [0.0, 0.0]\ny  = [0.0]\nby = [1.0];\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme stores the value and adjoint of a variable in an object of type Duplicated where the first element represent the value and the second the adjoint. Evaluating the reverse model using Enzyme is done via the following call.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme.autodiff(Reverse, f, Duplicated(x, bx), Duplicated(y, by));\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"This yields the gradient of f in bx at point x = [2.0, 2.0]. by is called the seed and has to be set to 10 in order to compute the gradient. Let's save the gradient for later.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g = copy(bx)","category":"page"},{"location":"generated/autodiff/#Forward-mode","page":"AutoDiff API","title":"Forward mode","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The forward model in AD is defined as","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"beginaligned\ny = f(x) \ndoty = nabla f(x) cdot x\nendaligned","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"To obtain the first element of the gradient using the forward model we have to seed dotx with dotx = 1000","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"x  = [2.0, 2.0]\ndx = [1.0, 0.0]\ny  = [0.0]\ndy = [0.0];\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"In the forward mode the second element of Duplicated stores the tangent.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme.autodiff(Forward, f, Duplicated(x, dx), Duplicated(y, dy));\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"We can now verify that indeed the reverse mode and forward mode yield the same result for the first component of the gradient. Note that to acquire the full gradient one needs to execute the forward model a second time with the seed dx set to [0.0,1.0].","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Let's verify whether the reverse and forward model agree.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g[1] == dy[1]","category":"page"},{"location":"generated/autodiff/#Forward-over-reverse","page":"AutoDiff API","title":"Forward over reverse","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The forward over reverse (FoR) model is obtained by applying the forward model to the reverse model using the chain rule for the product in the adjoint statement.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"beginaligned\ny = f(x) \ndoty = f(x) cdot dotx \nbarx = bary cdot nabla f(x) \ndotbarx = bary cdot nabla^2 f(x) cdot dotx + dotbary cdot nabla f(x)\nendaligned","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"To obtain the first column/row of the Hessian nabla^2 f(x) we have to seed dotbary with 00, bary with 10 and dotx with 10 00.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"y = [0.0]\nx = [2.0, 2.0]\n\ndy = [0.0]\ndx = [1.0, 0.0]\n\nbx = [0.0, 0.0]\nby = [1.0]\ndbx = [0.0, 0.0]\ndby = [0.0]\n\nEnzyme.autodiff(\n    Forward,\n    (x,y) -> Enzyme.autodiff_deferred(Reverse, f, x, y),\n    Duplicated(Duplicated(x, bx), Duplicated(dx, dbx)),\n    Duplicated(Duplicated(y, by), Duplicated(dy, dby)),\n)","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The FoR model also computes the forward model from before, giving us again the first component of the gradient.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g[1] == dy[1]","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"In addition we now have the first row/column of the Hessian.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"dbx[1] == 2.0\ndbx[2] == 1.0","category":"page"},{"location":"generated/autodiff/#Vector-forward-over-reverse","page":"AutoDiff API","title":"Vector forward over reverse","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The vector FoR allows us to propagate several tangents at once through the second-order model. This allows us the acquire the Hessian in one autodiff call. The multiple tangents are organized in tuples. Following the same seeding strategy as before, we now seed both in the vdx[1]=[1.0, 0.0] and vdx[2]=[0.0, 1.0] direction. These tuples have to be put into a BatchDuplicated type.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"y = [0.0]\nx = [2.0, 2.0]\n\nvdy = ([0.0],[0.0])\nvdx = ([1.0, 0.0], [0.0, 1.0])\n\nbx = [0.0, 0.0]\nby = [1.0]\nvdbx = ([0.0, 0.0], [0.0, 0.0])\nvdby = ([0.0], [0.0]);\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The BatchedDuplicated objects are constructed using the broadcast operator on our tuples of Duplicated for the tangents.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme.autodiff(\n    Forward,\n    (x,y) -> Enzyme.autodiff_deferred(Reverse, f, x, y),\n    BatchDuplicated(Duplicated(x, bx), Duplicated.(vdx, vdbx)),\n    BatchDuplicated(Duplicated(y, by), Duplicated.(vdy, vdby)),\n);\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Again we obtain the first-order gradient.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g[1] == vdy[1][1]","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"We have now the first row/column of the Hessian","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"vdbx[1][1] == 2.0\n\nvdbx[1][2] == 1.0","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"as well as the second row/column","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"vdbx[2][1] == 1.0\n\nvdbx[2][2] == 0.0","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Types-and-constants","page":"API","title":"Types and constants","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [Enzyme, EnzymeCore, EnzymeCore.EnzymeRules]\nOrder = [:type, :constant]","category":"page"},{"location":"api/#Functions-and-macros","page":"API","title":"Functions and macros","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [Enzyme, EnzymeCore, EnzymeCore.EnzymeRules]\nOrder = [:macro, :function]","category":"page"},{"location":"api/#Documentation","page":"API","title":"Documentation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [Enzyme, EnzymeCore, EnzymeCore.EnzymeRules]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"api/#Enzyme.gradient!-Tuple{EnzymeCore.ReverseMode, Any, Any, Any}","page":"API","title":"Enzyme.gradient!","text":"gradient!(::ReverseMode, dx, f, x)\n\nCompute the gradient of an array-input function f using reverse mode, storing the derivative result in an existing array dx.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ndx = [0.0, 0.0]\ngradient!(Reverse, dx, f, [2.0, 3.0])\n\n# output\n\n2-element Vector{Float64}:\n 3.0\n 2.0\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.gradient-Tuple{EnzymeCore.ForwardMode, Any, Any}","page":"API","title":"Enzyme.gradient","text":"gradient(::ForwardMode, f, x; shadow=onehot(x))\n\nCompute the gradient of an array-input function f using forward mode. The optional keyword argument shadow is a vector of one-hot vectors of type x which are used to forward-propagate into the return. For performance reasons, this should be computed once, outside the call to gradient, rather than within this call.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ngrad = gradient(Forward, f, [2.0, 3.0])\n\n# output\n\n(3.0, 2.0)\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.gradient-Tuple{EnzymeCore.ReverseMode, Any, Any}","page":"API","title":"Enzyme.gradient","text":"gradient(::ReverseMode, f, x)\n\nCompute the gradient of an array-input function f using reverse mode. This will allocate and return new array with the gradient result.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ngrad = gradient(Reverse, f, [2.0, 3.0])\n\n# output\n\n2-element Vector{Float64}:\n 3.0\n 2.0\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.gradient-Union{Tuple{chunk}, Tuple{X}, Tuple{F}, Tuple{EnzymeCore.ForwardMode, F, X, Val{chunk}}} where {F, X, chunk}","page":"API","title":"Enzyme.gradient","text":"gradient(::ForwardMode, f, x, ::Val{chunk}; shadow=onehot(x))\n\nCompute the gradient of an array-input function f using vector forward mode. Like gradient, except it uses a chunk size of chunk to compute chunk derivatives in a single call.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ngrad = gradient(Forward, f, [2.0, 3.0], Val(2))\n\n# output\n\n(3.0, 2.0)\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.jacobian-Tuple{EnzymeCore.ForwardMode, Any, Any}","page":"API","title":"Enzyme.jacobian","text":"jacobian(::ForwardMode, f, x; shadow=onehot(x))\njacobian(::ForwardMode, f, x, ::Val{chunk}; shadow=onehot(x))\n\nCompute the jacobian of an array-input function f using (potentially vector) forward mode. This is a simple rename of the gradient function, and all relevant arguments apply here.\n\nExample:\n\nf(x) = [x[1]*x[2], x[2]]\n\ngrad = jacobian(Forward, f, [2.0, 3.0])\n\n# output\n\n2×2 Matrix{Float64}:\n 3.0  2.0\n 0.0  1.0\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.jacobian-Union{Tuple{n_out_val}, Tuple{chunk}, Tuple{X}, Tuple{F}, Tuple{EnzymeCore.ReverseMode, F, X, Val{n_out_val}, Val{chunk}}} where {F, X, chunk, n_out_val}","page":"API","title":"Enzyme.jacobian","text":"jacobian(::ReverseMode, f, x, ::Val{num_outs}, ::Val{chunk})\n\nCompute the jacobian of an array-input function f using (potentially vector) reverse mode. The chunk argument denotes the chunk size to use and num_outs denotes the number of outputs f will return in an array.\n\nExample:\n\nf(x) = [x[1]*x[2], x[2]]\n\ngrad = jacobian(Reverse, f, [2.0, 3.0], Val(2))\n\n# output\n\n2×2 Matrix{Float64}:\n 3.0  2.0\n 0.0  1.0\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{F}, Tuple{EnzymeCore.ForwardMode, F, Type{A}, Vararg{Any}}} where {F, A<:EnzymeCore.Annotation}","page":"API","title":"EnzymeCore.autodiff","text":"autodiff(::ForwardMode, f, Activity, args...)\n\nAuto-differentiate function f at arguments args using forward mode.\n\nargs may be numbers, arrays, structs of numbers, structs of arrays and so on. Enzyme will only differentiate in respect to arguments that are wrapped in a Duplicated or similar argument. Non-annotated arguments will automatically be treated as Const. Unlike reverse mode in autodiff, Active arguments are not allowed here, since all \n\nActivity is the Activity of the return value, it may be:\n\nConst if the return is not to be differentiated with respect to\nDuplicated, if the return is being differentiated with respect to and both the original value and the derivative return are desired\nDuplicatedNoNeed, if the return is being differentiated with respect to and only the derivative return is desired.\n\nExample returning both original return and derivative:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(x) = x*x\nres, ∂f_∂x = autodiff(Forward, f, Duplicated, Duplicated(3.14, 1.0))\n\n# output\n\n(9.8596, 6.28)\n\nExample returning just the derivative:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(x) = x*x\n∂f_∂x = autodiff(Forward, f, DuplicatedNoNeed, Duplicated(3.14, 1.0))\n\n# output\n\n(6.28,)\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff-Union{Tuple{CMode}, Tuple{F}, Tuple{CMode, F, Vararg{Any}}} where {F, CMode<:EnzymeCore.Mode}","page":"API","title":"EnzymeCore.autodiff","text":"autodiff(mode::Mode, f, args...)\n\nLike autodiff but will try to guess the activity of the return value.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff-Union{Tuple{ReturnPrimal}, Tuple{A}, Tuple{F}, Tuple{EnzymeCore.ReverseMode{ReturnPrimal}, F, Type{A}, Vararg{Any}}} where {F, A<:EnzymeCore.Annotation, ReturnPrimal}","page":"API","title":"EnzymeCore.autodiff","text":"autodiff(::ReverseMode, f, Activity, args...)\n\nAuto-differentiate function f at arguments args using reverse mode.\n\nLimitations:\n\nf may only return a Real (of a built-in/primitive type) or nothing, not an array, struct, BigFloat, etc. To handle vector-valued return types, use a mutating f! that returns nothing and stores it's return value in one of the arguments, which must be wrapped in a Duplicated.\n\nargs may be numbers, arrays, structs of numbers, structs of arrays and so on. Enzyme will only differentiate in respect to arguments that are wrapped in an Active (for immutable arguments like primitive types and structs thereof) or Duplicated (for mutable arguments like arrays, Refs and structs thereof). Non-annotated arguments will automatically be treated as Const.\n\nActivity is the Activity of the return value, it may be Const or Active.\n\nExample:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(a, b, c, d) = a * √(b[1]^2 + b[2]^2) + c^2 * d^2\n∂f_∂a, _, _, ∂f_∂d = autodiff(Reverse, f, Active, Active(a), Duplicated(b, ∂f_∂b), c, Active(d))[1]\n\n# output\n\n(3.966106403010388, nothing, nothing, 54450.0)\n\nhere, autodiff returns a tuple (partial fpartial a partial fpartial d), while partial fpartial b will be added to ∂f_∂b (but not returned). c will be treated as Const(c).\n\nOne can also request the original returned value of the computation.\n\nExample:\n\nEnzyme.autodiff(ReverseWithPrimal, x->x*x, Active(3.0))\n\n# output\n\n((6.0,), 9.0)\n\nnote: Note\nEnzyme gradients with respect to integer values are zero. Active will automatically convert plain integers to floating point values, but cannot do so for integer values in tuples and structs.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_deferred-Union{Tuple{A}, Tuple{F}, Tuple{EnzymeCore.ForwardMode, F, Type{A}, Vararg{Any}}} where {F, A<:EnzymeCore.Annotation}","page":"API","title":"EnzymeCore.autodiff_deferred","text":"autodiff_deferred(::ForwardMode, f, Activity, args...)\n\nSame as autodiff(::ForwardMode, ...) but uses deferred compilation to support usage in GPU code, as well as high-order differentiation.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_deferred-Union{Tuple{ReturnPrimal}, Tuple{A}, Tuple{F}, Tuple{EnzymeCore.ReverseMode{ReturnPrimal}, F, Type{A}, Vararg{Any}}} where {F, A<:EnzymeCore.Annotation, ReturnPrimal}","page":"API","title":"EnzymeCore.autodiff_deferred","text":"autodiff_deferred(::ReverseMode, f, Activity, args...)\n\nSame as autodiff but uses deferred compilation to support usage in GPU code, as well as high-order differentiation.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_deferred-Union{Tuple{SMode}, Tuple{F}, Tuple{SMode, F, Vararg{Any}}} where {F, SMode<:EnzymeCore.Mode}","page":"API","title":"EnzymeCore.autodiff_deferred","text":"autodiff_deferred(mode, f, args...)\n\nLike autodiff_deferred but will try to guess the activity of the return value.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_thunk-Union{Tuple{ModifiedBetweenT}, Tuple{Width}, Tuple{ReturnShadow}, Tuple{ReturnPrimal}, Tuple{A}, Tuple{F}, Tuple{EnzymeCore.ReverseModeSplit{ReturnPrimal, ReturnShadow, Width, ModifiedBetweenT}, F, Type{A}, Vararg{Any}}} where {F, A<:EnzymeCore.Annotation, ReturnPrimal, ReturnShadow, Width, ModifiedBetweenT}","page":"API","title":"EnzymeCore.autodiff_thunk","text":"autodiff_thunk(::ReverseModeSplit, f, Activity, argtypes...)\n\nProvide the split forward and reverse pass functions for function f when called with args of type argtypes when using reverse mode.\n\nActivity is the Activity of the return value, it may be Const, Active, or Duplicated (or its variants DuplicatedNoNeed, BatchDuplicated, and BatchDuplicatedNoNeed).\n\nThe forward function will return a tape, the primal (or nothing if not requested), and the shadow (or nothing if not a Duplicated variant), and tapes the corresponding type arguements provided.\n\nThe reverse function will return the derivative of Active arguments, updating the Duplicated arguments in place. The same arguments to the forward pass should be provided, followed by the adjoint of the return (if the return is active), and finally the tape from the forward pass.\n\nExample:\n\n\nA = [2.2]; ∂A = zero(A)\nv = 3.3\n\nfunction f(A, v)\n    res = A[1] * v\n    A[1] = 0\n    res\nend\n\nforward, reverse = autodiff_thunk(ReverseSplitWithPrimal, f, Active, Duplicated{typeof(A)}, Active{typeof(v)})\n\ntape, result, shadow_result  = forward(Duplicated(A, ∂A), Active(v))\n_, ∂v = reverse(Duplicated(A, ∂A), Active(v), 1.0, tape)[1]\n\nresult, ∂v, ∂A \n\n# output\n\n(7.26, 2.2, [3.3])\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.Active","page":"API","title":"EnzymeCore.Active","text":"Active(x)\n\nMark a function argument x of autodiff as active, Enzyme will auto-differentiate in respect Active arguments.\n\nnote: Note\nEnzyme gradients with respect to integer values are zero. Active will automatically convert plain integers to floating point values, but cannot do so for integer values in tuples and structs.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Annotation","page":"API","title":"EnzymeCore.Annotation","text":"abstract type Annotation{T}\n\nAbstract type for autodiff function argument wrappers like Const, Active and Duplicated.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.BatchDuplicated","page":"API","title":"EnzymeCore.BatchDuplicated","text":"BatchDuplicated(x, ∂f_∂xs)\n\nLike Duplicated, except contains several shadows to compute derivatives for all at once. Argument ∂f_∂xs should be a tuple of the several values of type x.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.BatchDuplicatedNoNeed","page":"API","title":"EnzymeCore.BatchDuplicatedNoNeed","text":"BatchDuplicatedNoNeed(x, ∂f_∂xs)\n\nLike DuplicatedNoNeed, except contains several shadows to compute derivatives for all at once. Argument ∂f_∂xs should be a tuple of the several values of type x.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Const","page":"API","title":"EnzymeCore.Const","text":"Const(x)\n\nMark a function argument x of autodiff as constant, Enzyme will not auto-differentiate in respect Const arguments.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Duplicated","page":"API","title":"EnzymeCore.Duplicated","text":"Duplicated(x, ∂f_∂x)\n\nMark a function argument x of autodiff as duplicated, Enzyme will auto-differentiate in respect to such arguments, with dx acting as an accumulator for gradients (so partial f  partial x will be added to) ∂f_∂x.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.DuplicatedNoNeed","page":"API","title":"EnzymeCore.DuplicatedNoNeed","text":"DuplicatedNoNeed(x, ∂f_∂x)\n\nLike Duplicated, except also specifies that Enzyme may avoid computing the original result and only compute the derivative values.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.ForwardMode","page":"API","title":"EnzymeCore.ForwardMode","text":"struct Forward <: Mode\n\nForward mode differentiation\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Mode","page":"API","title":"EnzymeCore.Mode","text":"abstract type Mode\n\nAbstract type for what differentiation mode will be used.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.ReverseMode","page":"API","title":"EnzymeCore.ReverseMode","text":"struct ReverseMode{ReturnPrimal} <: Mode\n\nReverse mode differentiation.\n\nReturnPrimal: Should Enzyme return the primal return value from the augmented-forward.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.ReverseModeSplit","page":"API","title":"EnzymeCore.ReverseModeSplit","text":"struct ReverseModeSplit{ReturnPrimal,ReturnShadow,Width,ModifiedBetween} <: Mode\n\nReverse mode differentiation.\n\nReturnPrimal: Should Enzyme return the primal return value from the augmented-forward.\nReturnShadow: Should Enzyme return the shadow return value from the augmented-forward.\nWidth: Batch Size (0 if to be automatically derived)\nModifiedBetween: Tuple of each argument's modified between state (true if to be automatically derived).\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.congruent","page":"API","title":"EnzymeCore.congruent","text":"congruent(a::T, b::T)::T\n\nDefines values to be congruent, e.g. structurally equivalent.\n\n\n\n\n\n","category":"function"},{"location":"api/#EnzymeCore.structure_check!-Tuple{Any}","page":"API","title":"EnzymeCore.structure_check!","text":"structure_check!(flag)\n\nToggle the default setting for congruence/structure checking.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.EnzymeRules.augmented_primal","page":"API","title":"EnzymeCore.EnzymeRules.augmented_primal","text":"augmented_primal(::Config, func::Annotation{typeof(f)}, RT::Type{<:Annotation}, args::Annotation...)\n\nMust return an AugmentedReturn type.\n\nThe primal must be the same type of the original return if needs_primal(config), otherwise nothing.\nThe shadow must be nothing if needs_shadow(config) is false. If width is 1, the shadow should be the same type of the original return. If the width is greater than 1, the shadow should be NTuple{original return, width}.\nThe tape can be any type (including Nothing) and is preserved for the reverse call.\n\n\n\n\n\n","category":"function"},{"location":"api/#EnzymeCore.EnzymeRules.forward","page":"API","title":"EnzymeCore.EnzymeRules.forward","text":"forward(func::Annotation{typeof(f)}, RT::Type{<:Annotation}, args::Annotation...)\n\nCalculate the forward derivative. The first argument func is the callable for which the rule applies to. Either wrapped in a Const), or a Duplicated if it is a closure. The second argument is the return type annotation, and all other arguments are the annotated function arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#EnzymeCore.EnzymeRules.inactive","page":"API","title":"EnzymeCore.EnzymeRules.inactive","text":"inactive(func::typeof(f), args::...)\n\nMark a particular function as always being inactive in both its return result and the function call itself.\n\n\n\n\n\n","category":"function"},{"location":"api/#EnzymeCore.EnzymeRules.reverse","page":"API","title":"EnzymeCore.EnzymeRules.reverse","text":"reverse(::Config, func::Annotation{typeof(f)}, dret::Active, tape, args::Annotation...)\nreverse(::Config, func::Annotation{typeof(f)}, ::Type{<:Annotation), tape, args::Annotation...)\n\nTakes gradient of derivative, activity annotation, and tape. If there is an active return dret is passed as Active{T} with the active return val. Otherwise dret is passed as Type{Duplicated{T}}, etc.\n\n\n\n\n\n","category":"function"},{"location":"pullbacks/#Implementing-pullbacks","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"","category":"section"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Enzyme's autodiff function can only handle functions with scalar output. To implement pullbacks (back-propagation of gradients/tangents) for array-valued functions, use a mutating function that returns nothing and stores it's result in one of the arguments, which must be passed wrapped in a Duplicated.","category":"page"},{"location":"pullbacks/#Example","page":"Implementing pullbacks","title":"Example","text":"","category":"section"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Given a function mymul! that performs the equivalent of R = A * B for matrices A and B, and given a gradient (tangent) ∂z_∂R, we can compute ∂z_∂A and ∂z_∂B like this:","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"using Enzyme, Random\n\nfunction mymul!(R, A, B)\n    @assert axes(A,2) == axes(B,1)\n    @inbounds @simd for i in eachindex(R)\n        R[i] = 0\n    end\n    @inbounds for j in axes(B, 2), i in axes(A, 1)\n        @inbounds @simd for k in axes(A,2)\n            R[i,j] += A[i,k] * B[k,j]\n        end\n    end\n    nothing\nend\n\nRandom.seed!(1234)\nA = rand(5, 3)\nB = rand(3, 7)\n\nR = zeros(size(A,1), size(B,2))\n∂z_∂R = rand(size(R)...)  # Some gradient/tangent passed to us\n∂z_∂R0 = copyto!(similar(∂z_∂R), ∂z_∂R)  # exact copy for comparison\n\n∂z_∂A = zero(A)\n∂z_∂B = zero(B)\n\nEnzyme.autodiff(Reverse, mymul!, Const, Duplicated(R, ∂z_∂R), Duplicated(A, ∂z_∂A), Duplicated(B, ∂z_∂B))","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Now we have:","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"R ≈ A * B            &&\n∂z_∂A ≈ ∂z_∂R0 * B'   &&  # equivalent to Zygote.pullback(*, A, B)[2](∂z_∂R)[1]\n∂z_∂B ≈ A' * ∂z_∂R0       # equivalent to Zygote.pullback(*, A, B)[2](∂z_∂R)[2]","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Note that the result of the backpropagation is added to ∂z_∂A and ∂z_∂B, they act as accumulators for gradient information.","category":"page"},{"location":"dev_docs/#Enzyme-developer-documentation","page":"For developers","title":"Enzyme developer documentation","text":"","category":"section"},{"location":"dev_docs/#Development-of-Enzyme-and-Enzyme.jl-together","page":"For developers","title":"Development of Enzyme and Enzyme.jl together","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Normally Enzyme.jl downloads and install Enzyme for the user automatically since Enzyme needs to be built against Julia bundeled LLVM. In case that you are making updates to Enzyme and want to test them against Enzyme.jl the instructions below should help you get started.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Start Julia in your development copy of Enzyme.jl","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"~/s/Enzyme (master)> julia --project=.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Then create a development copy of Enzyme_jll and activate it within.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"julia> using Enzyme_jll\njulia> Enzyme_jll.dev_jll()\n[ Info: Enzyme_jll dev'ed out to ${JULIA_PKG_DEVDIR}/Enzyme_jll with pre-populated override directory\n(Enzyme) pkg> dev Enzyme_jll\nPath `${JULIA_PKG_DEVDIR}/Enzyme_jll` exists and looks like the correct package. Using existing path.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"After restarting Julia:","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"julia> Enzyme_jll.dev_jll()\njulia> Enzyme_jll.libEnzyme_path\n\"${JULIA_PKG_DEVDIR}/Enzyme_jll/override/lib/LLVMEnzyme-9.so\"","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"On your machine ${JULIA_PKG_DEVDIR} most likely corresponds to ~/.julia/dev. Now we can inspect \"${JULIA_PKG_DEVDIR}/Enzyme_jll/override/lib and see that there is a copy of LLVMEnzyme-9.so, which we can replace with a symbolic link or a copy of a version of Enzyme.","category":"page"},{"location":"dev_docs/#Building-Enzyme-against-Julia's-LLVM.","page":"For developers","title":"Building Enzyme against Julia's LLVM.","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Depending on how you installed Julia the LLVM Julia is using will be different.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Download from julialang.org (Recommended)\nManual build on your machine\nUses a pre-built Julia from your system vendor (Not recommended)","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"To check what LLVM Julia is using use:","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"julia> Base.libllvm_version_string\n\"9.0.1jl\"","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"If the LLVM version ends in a jl you a likely using the private LLVM.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"In your source checkout of Enzyme:","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"mkdir build-jl\ncd build-jl","category":"page"},{"location":"dev_docs/#Prebuilt-binary-from-julialang.org","page":"For developers","title":"Prebuilt binary from julialang.org","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"LLVM_MAJOR_VER=`julia -e \"print(Base.libllvm_version.major)\"`\njulia -e \"using Pkg; pkg\\\"add LLVM_full_jll@${LLVM_MAJOR_VER}\\\"\"\nLLVM_DIR=`julia -e \"using LLVM_full_jll; print(LLVM_full_jll.artifact_dir)\"`\necho \"LLVM_DIR=$LLVM_DIR\"\ncmake ../enzyme/ -G Ninja -DENZYME_EXTERNAL_SHARED_LIB=ON -DLLVM_DIR=${LLVM_DIR} -DLLVM_EXTERNAL_LIT=${LLVM_DIR}/tools/lit/lit.py","category":"page"},{"location":"dev_docs/#Manual-build-of-Julia","page":"For developers","title":"Manual build of Julia","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"cmake ../enzyme/ -G Ninja -DENZYME_EXTERNAL_SHARED_LIB=ON -DLLVM_DIR=${PATH_TO_BUILDDIR_OF_JULIA}/usr/lib/cmake/llvm/","category":"page"},{"location":"internal_api/#Internal-API","page":"Internal API","title":"Internal API","text":"","category":"section"},{"location":"internal_api/","page":"Internal API","title":"Internal API","text":"note: Note\nThis is the documentation of Enzymes's internal API. The internal API is not subject to semantic versioning and may change at any time and without deprecation.","category":"page"},{"location":"internal_api/","page":"Internal API","title":"Internal API","text":"Modules = [Enzyme.Compiler]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"internal_api/#Enzyme.Compiler.fspec-Tuple{Any, Any}","page":"Internal API","title":"Enzyme.Compiler.fspec","text":"Create the FunctionSpec pair, and lookup the primal return type.\n\n\n\n\n\n","category":"method"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"EditURL = \"https://github.com/EnzymeAD/Enzyme.jl/blob/main/examples/box.jl\"","category":"page"},{"location":"generated/box/#Enzyme-for-adjoint-tutorial:-Stommel-three-box-ocean-model","page":"Box model","title":"Enzyme for adjoint tutorial: Stommel three-box ocean model","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"The goal of this tutorial is to teach about a specific usage of Enzyme's automatic differentiation capabilities, and will be centered around the Stommel ocean model. This is a nice example to see how powerful Enzyme is, and the ability of it to take a derivative of a complicated function (namely one that has many parts and parameters). This tutorial will focus first on the computations and getting Enzyme running, for those interested a mathematical explanation of the model and what an adjoint variable is will be provided at the end.","category":"page"},{"location":"generated/box/#Brief-model-overview","page":"Box model","title":"Brief model overview","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"The Stommel box model can be viewed as a watered down full ocean model. In our example, we have three boxes (Box One, Box Two, and Box Three) and we model the transport of fluid between them. The full equations of our system are given by:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"beginaligned\n   U = u_0 left rho_2 - left rho_1 + (1 - delta) rho_3 right right \n   rho_i = -alpha T_i + beta S_i     i = 1 2 3\nendaligned","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"for the transport U and densities rho, and then the time derivatives","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"beginaligned\n   dotT_1 = U(T_3 - T_1)V_1 + gamma (T_1^* - T_1 )  dotS_1 = U(S_3 - S_1)V_1 + FW_1V_1 \n   dotT_2 = U(T_1 - T_2)V_2 + gamma (T_2^* - T_2 )  dotS_2 = U(S_1 - S_2)V_2 + FW_2V_2 \n   dotT_3 = U(T_2 - T_3)V_3  dotS_3 = U(S_2 - S_3)V_3\nendaligned","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"for positive transport, U  0, and","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"beginaligned\n   dotT_1 = U(T_2 - T_1)V_1 + gamma (T_1^* - T_1)  dotS_1 = U(S_2 - S_1)V_1 + FW_1V_1 \n   dotT_2 = U(T_3 - T_2)V_2 + gamma (T_2^* - T_2 )  dotS_2 = U(S_3 - S_2)V_2 + FW_2V_2 \n   dotT_3 = U(T_1 - T_3)V_3  dotS_3 = U(S_1 - S_3)V_3\nendaligned","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"for U leq 0. The only force driving our system is a density gradient generated via temperature and salinity differences between the boxes. This makes it a really easy model to play around with! With this in mind, the model is run forward with the steps:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Compute densities\nCompute transport\nCompute time derivatives of the box temperatures and salinities\nUpdate the state vector","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"We'll start by going through the model setup step by step, then providing a few test cases with Enzyme.","category":"page"},{"location":"generated/box/#Model-setup","page":"Box model","title":"Model setup","text":"","category":"section"},{"location":"generated/box/#Model-dependencies","page":"Box model","title":"Model dependencies","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Let's first add the necessary packages to run everything","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"using Enzyme","category":"page"},{"location":"generated/box/#Initialize-constants","page":"Box model","title":"Initialize constants","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"The system equations have quite a few constants that appear, here we initialize them for later use","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"const blength = [5000.0e5; 1000.0e5; 5000.0e5]   ## north-south size of boxes, centimeters\n\nconst bdepth = [1.0e5; 5.0e5; 4.0e5]   ## depth of boxes, centimeters\n\nconst delta = bdepth[1]/(bdepth[1] + bdepth[3])  ## constant ratio of two depths\n\nconst bwidth = 4000.0*1e5  ## box width, centimeters\n\n# box areas\nconst barea = [blength[1]*bwidth;\n         blength[2]*bwidth;\n         blength[3]*bwidth]\n\n# box volumes\nconst bvol = [barea[1]*bdepth[1];\n        barea[2]*bdepth[2];\n        barea[3]*bdepth[3]]\n\n# parameters that are used to ensure units are in CGS (cent-gram-sec)\n\nconst hundred = 100.0\nconst thousand = 1000.0\nconst day = 3600.0*24.0\nconst year = day*365.0\nconst Sv = 1e12     ## one Sverdrup (a unit of ocean transport), 1e6 meters^3/second\n\n# parameters that appear in box model equations\nconst u0 = 16.0*Sv/0.0004\nconst alpha = 1668e-7\nconst beta = 0.7811e-3\n\nconst gamma = 1/(300*day)\n\n# robert filter coefficient for the smoother part of the timestep\nconst robert_filter_coeff = 0.25\n\n# freshwater forcing\nconst FW = [(hundred/year) * 35.0 * barea[1]; -(hundred/year) * 35.0 * barea[1]]\n\n# restoring atmospheric temperatures\nconst Tstar = [22.0; 0.0]\nconst Sstar = [36.0; 34.0];\nnothing #hide","category":"page"},{"location":"generated/box/#Define-model-functions","page":"Box model","title":"Define model functions","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Here we define functions that will calculate quantities used in the forward steps.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"# function to compute transport\n#       Input: rho - the density vector\n#       Output: U - transport value\n\nfunction U_func(dens)\n\n    U = u0*(dens[2] - (delta * dens[1] + (1 - delta)*dens[3]))\n    return U\n\nend\n\n# function to compute density\n#       Input: state = [T1; T2; T3; S1; S2; S3]\n#       Output: rho\n\nfunction rho_func(state)\n\n    rho = zeros(3)\n\n    rho[1] = -alpha * state[1] + beta * state[4]\n    rho[2] = -alpha * state[2] + beta * state[5]\n    rho[3] = -alpha * state[3] + beta * state[6]\n\n    return rho\n\nend\n\n# lastly our timestep function\n#       Input: fld_now = [T1(t), T2(t), ..., S3(t)]\n#           fld_old = [T1(t-dt), ..., S3(t-dt)]\n#           u = transport(t)\n#           dt = time step\n#       Output: fld_new = [T1(t+dt), ..., S3(t+dt)]\n\nfunction timestep_func(fld_now, fld_old, u, dt)\n\n    temp = zeros(6)\n    fld_new = zeros(6)\n\n    # first computing the time derivatives of the various temperatures and salinities\n    if u > 0\n\n        temp[1] = u * (fld_now[3] - fld_now[1]) / bvol[1] + gamma * (Tstar[1] - fld_now[1])\n        temp[2] = u * (fld_now[1] - fld_now[2]) / bvol[2] + gamma * (Tstar[2] - fld_now[2])\n        temp[3] = u * (fld_now[2] - fld_now[3]) / bvol[3]\n\n        temp[4] = u * (fld_now[6] - fld_now[4]) / bvol[1] + FW[1] / bvol[1]\n        temp[5] = u * (fld_now[4] - fld_now[5]) / bvol[2] + FW[2] / bvol[2]\n        temp[6] = u * (fld_now[5] - fld_now[6]) / bvol[3]\n\n    elseif u <= 0\n\n        temp[1] = u * (fld_now[2] - fld_now[1]) / bvol[1] + gamma * (Tstar[1] - fld_now[1])\n        temp[2] = u * (fld_now[3] - fld_now[2]) / bvol[2] + gamma * (Tstar[2] - fld_now[2])\n        temp[3] = u * (fld_now[1] - fld_now[3]) / bvol[3]\n\n        temp[4] = u * (fld_now[5] - fld_now[4]) / bvol[1] + FW[1] / bvol[1]\n        temp[5] = u * (fld_now[6] - fld_now[5]) / bvol[2] + FW[2] / bvol[2]\n        temp[6] = u * (fld_now[4] - fld_now[6]) / bvol[3]\n\n    end\n\n    # update fldnew using a version of Euler's method\n\n    for j = 1:6\n        fld_new[j] = fld_old[j] + 2.0 * dt * temp[j]\n    end\n\n    return fld_new\nend","category":"page"},{"location":"generated/box/#Define-forward-functions","page":"Box model","title":"Define forward functions","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Finally, we create two functions, the first of which computes and stores all the states of the system, and the second which has been written specifically to be passed to Enzyme.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Let's start with the standard forward function. This is just going to be used to store the states at every timestep:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"function forward_func(fld_old, fld_now, dt, M)\n\n    state_now = copy(fld_now)\n    state_old = copy(fld_old)\n    state_new = zeros(6)\n\n    states_unsmooth = [state_old];\n    states_smooth = [state_old]\n\n    for t = 1:M\n        rho_now = rho_func(state_now)\n        u_now = U_func(rho_now)\n        state_new = timestep_func(state_now, state_old, u_now, dt)\n\n        # Robert filter smoother (needed for stability)\n        for j = 1:6\n            state_now[j] = state_now[j] + robert_filter_coeff * (state_new[j] - 2.0 * state_now[j] + state_old[j])\n        end\n\n        push!(states_smooth, copy(state_now))\n        push!(states_unsmooth, copy(state_new))\n\n        # cycle the \"now, new, old\" states\n\n        state_old = state_now\n        state_now = state_new\n    end\n\n    return states_smooth, states_unsmooth\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Next, we have the Enzyme-designed forward function. This is what we'll actually be passing to Enzyme to differentiate:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"function forward_func_4_AD(in_now, in_old, out_old, out_now)\n\n    rho_now = rho_func(in_now)                             ## compute density\n    u_now = U_func(rho_now)                                ## compute transport\n    in_new = timestep_func(in_now, in_old, u_now, 10*day)  ## compute new state values\n\n    # Robert filter smoother\n    in_now[1] = in_now[1] + robert_filter_coeff * (in_new[1] - 2.0 * in_now[1] + in_old[1])\n    in_now[2] = in_now[2] + robert_filter_coeff * (in_new[2] - 2.0 * in_now[2] + in_old[2])\n    in_now[3] = in_now[3] + robert_filter_coeff * (in_new[3] - 2.0 * in_now[3] + in_old[3])\n    in_now[4] = in_now[4] + robert_filter_coeff * (in_new[4] - 2.0 * in_now[4] + in_old[4])\n    in_now[5] = in_now[5] + robert_filter_coeff * (in_new[5] - 2.0 * in_now[5] + in_old[5])\n    in_now[6] = in_now[6] + robert_filter_coeff * (in_new[6] - 2.0 * in_now[6] + in_old[6])\n\n    out_old[:] = in_now\n    out_now[:] = in_new\n    return nothing\n\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Two key differences:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"forward_func_4_AD now returns nothing, but is rather a function of both its input and output.\nAll operations are now inlined, meaning we compute the entries of the input vector individually.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Currently, Enzyme does not have compatibility with matrix/vector operations so inlining is necessary to run Enzyme on this function.","category":"page"},{"location":"generated/box/#Example-1:-Simply-using-Enzyme","page":"Box model","title":"Example 1: Simply using Enzyme","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"For the first example let's just compute the gradient of our forward function and examine the output. We'll just run the model for one step, and take a dt of ten days. The initial conditions of the system are given as Tbar and Sbar.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"const Tbar = [20.0; 1.0; 1.0]\nconst Sbar = [35.5; 34.5; 34.5]\n\n# Running the model one step forward\nstates_smooth, states_unsmooth = forward_func(copy([Tbar; Sbar]), copy([Tbar; Sbar]), 10*day, 1)\n\n# Run Enzyme one time on `forward_func_4_AD``\ndin_now = zeros(6)\ndin_old = zeros(6)\nout_now = zeros(6); dout_now = ones(6)\nout_old = zeros(6); dout_old = ones(6)\nautodiff(Reverse, forward_func_4_AD, Duplicated([Tbar; Sbar], din_now), Duplicated([Tbar; Sbar], din_old),\n                    Duplicated(out_now, dout_now), Duplicated(out_old, dout_old));\nnothing #hide","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"In order to run Enzyme on forward_func_4_AD, we've needed to provide quite a few placeholders, and wrap everything in Duplicated as all components of our function are vectors, not scalars. Let's go through and see what Enzyme did with all of those placeholders.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"First we can look at what happened to the zero vectors outnow and outold:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show out_now, out_old","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Comparing to the results of forward func:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show states_smooth[2], states_unsmooth[2]","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"we see that Enzyme has computed and stored exactly the output of the forward step. Next, let's look at din_now:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show din_now","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Just a few numbers, but this is what makes AD so nice: Enzyme has exactly computed the derivative of all outputs with respect to the input innow, evaluated at innow, and acted with this gradient on what we gave as dout_now (in our case, all ones). In math language, this is just","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"textdin now = (fracpartial textout now(textin now)partial textin now + fracpartial textout old(textin now)partial textin now) textdout now","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"We note here that had we given doutnow and doutnow as something else, our results will change. Let's multiply them by two and see what happens.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"din_now_new = zeros(6)\ndin_old_new = zeros(6)\nout_now = zeros(6); dout_now = 2*ones(6)\nout_old = zeros(6); dout_old = 2*ones(6)\nautodiff(Reverse, forward_func_4_AD, Duplicated([Tbar; Sbar], din_now_new), Duplicated([Tbar; Sbar], din_old_new),\n                    Duplicated(out_now, dout_now), Duplicated(out_old, dout_old));\nnothing #hide","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Now checking dinnownew and dinoldnew we see","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show din_now_new","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"What happened? Enzyme is actually taking the computed gradient and acting on what we give as input to doutnow and doutold. Checking this, we see","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show 2*din_now","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"and they match the new results.","category":"page"},{"location":"generated/box/#Example-2:-Full-sensitivity-calculations","page":"Box model","title":"Example 2: Full sensitivity calculations","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Now we want to use Enzyme for a bit more than just a single derivative. Let's say we'd like to understand how sensitive the final temperature of Box One is to the initial salinity of Box Two. That is, given the function","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"J = (100000)^T cdot mathbfx(t_f)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"we want Enzyme to calculate the derivative","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"fracpartial Jpartial mathbfx(0)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"where x(t) is the state of the model at time t. If we think about x(t_f) as solely depending on the initial condition, then this derivative is really","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"fracpartial Jpartial mathbfx(0) = fracpartialpartial mathbfx(0) left( (100000)^T cdot L(ldots(L(mathbfx(0)))) right)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"with L(x(t)) = x(t + dt), i.e. one forward step. One could expand this derivative with the chain rule (and it would be very complicated), but really this is where Enzyme comes in. Each run of autodiff on our forward function is one piece of this big chain rule done for us! We also note that the chain rule goes from the outside in, so we start with the derivative of the forward function at the final state, and work backwards until the initial state. To get Enzyme to do this, we complete the following steps:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Run the forward model and store outputs (in a real ocean model this wouldn't be    feasible and we'd need to use checkpointing)\nCompute the initial derivative from the final state\nUse Enzyme to work backwards until we reach the desired derivative.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"For simplicity we define a function that takes completes our AD steps","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"function ad_calc(in_now, in_old, M)\n\ndout_old = [1.0;0.0;0.0;0.0;0.0;0.0]\ndout_now = [0.0;0.0;0.0;0.0;0.0;0.0]\n\nfor j = M:-1:1\n\n    din_now = zeros(6)\n    din_old = zeros(6)\n\n    autodiff(Reverse, forward_func_4_AD, Duplicated(in_now[j], din_now),\n            Duplicated(in_old[j], din_old), Duplicated(zeros(6), dout_old),\n            Duplicated(zeros(6), dout_now))\n\n    dout_old = copy(din_old)\n    dout_now = copy(din_now)\n\n    if j == 1\n        return din_now, din_old\n    end\n\nend\n\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"First we complete step one and run the forward model:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"const M = 10000             ## Deciding on total number of forward steps to take\n\nstates_smooth, states_unsmooth = forward_func(copy([Tbar; Sbar]), copy([Tbar; Sbar]), 10*day, M);\nnothing #hide","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Next, we pass all of our states to the AD function to get back to the desired derivative:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"adjoint_now, adjoint_old = ad_calc(states_unsmooth, states_smooth, M);\nnothing #hide","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"And we're done! We were interested in sensitivity to the initial salinity of box two, which will live in what we've called adjoint_old. Checking this value we see","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show adjoint_old[5]","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"As it stands this is just a number, but a good check that Enzyme has computed what we want is to approximate the derivative with a Taylor series. Specifically,","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"J(mathbfx(0) + varepsilon) approx J(mathbfx(0)) +\nvarepsilon fracpartial Jpartial mathbfx(0)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"and a simple rearrangement yields","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"fracpartial Jpartial mathbfx(0) approx\nfracJ(mathbfx(0) + varepsilon)  - J(mathbfx(0))varepsilon","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Hopefully we see that the analytical values converge close to the one we found with Enzyme:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"# unperturbed final state\nuse_to_check = states_smooth[M+1]\n\n# a loop to compute the perturbed final state\ndiffs = []\nstep_sizes = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]\nfor eps in step_sizes\n    new1 = Tbar\n    new2 = Sbar + [0.0;eps;0.0]\n    state_old = [new1; new2];\n    state_new = zeros(6);\n    state_now = [Tbar; Sbar];\n\n    for t = 1:M\n\n        rho_now = rho_func(state_now)\n        u_now = U_func(rho_now)\n        state_new = timestep_func(state_now, state_old, u_now, 10*day)\n\n        for j = 1:6\n            state_now[j] = state_now[j] + robert_filter_coeff * (state_new[j] - 2.0 * state_now[j] + state_old[j])\n        end\n\n        state_old = state_now\n        state_now = state_new\n\n\n    end\n\n    temp = (state_old[1] - use_to_check[1])/eps;\n    push!(diffs, temp)\n\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Then checking what we found the derivative to be analytically:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show diffs","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"which comes very close to our calculated value. We can go further and check the percent difference to see","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show abs.(diffs .- adjoint_old[5])./adjoint_old[5]","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"and we get down to a percent difference on the order of 1e^-5, showing Enzyme calculated the correct derivative. Success!","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Enzyme\nDocTestSetup = quote\n    using Enzyme\nend","category":"page"},{"location":"#Enzyme","page":"Home","title":"Enzyme","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Enzyme.jl, the Julia bindings for Enzyme.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Enzyme performs automatic differentiation (AD) of statically analyzable LLVM. It is highly-efficient and its ability perform AD on optimized code allows Enzyme to meet or exceed the performance of state-of-the-art AD tools.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Enzyme.jl can be installed in the usual way Julia packages are installed:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Enzyme","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Enzyme binary dependencies will be installed automatically via Julia's binary actifact system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Enzyme.jl API revolves around the function autodiff, see it's documentation for details and a usage example. Also see Implementing pullbacks on how to use Enzyme.jl to implement back-propagation for functions with non-scalar results.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> rosenbrock(x, y) = (1.0 - x)^2 + 100.0 * (y - x^2)^2\nrosenbrock (generic function with 1 method)\n\njulia> rosenbrock_inp(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nrosenbrock_inp (generic function with 1 method)","category":"page"},{"location":"#Reverse-mode","page":"Home","title":"Reverse mode","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The return value of reverse mode is a tuple that contains as a first value the derivative value of the active inputs and optionally the primal return value.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Reverse, rosenbrock, Active, Active(1.0), Active(2.0))\n((-400.0, 200.0),)\n\njulia> autodiff(ReverseWithPrimal, rosenbrock, Active, Active(1.0), Active(2.0))\n((-400.0, 200.0), 100.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> x = [1.0, 2.0]\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> dx = [0.0, 0.0]\n2-element Vector{Float64}:\n 0.0\n 0.0\n\njulia> autodiff(Reverse, rosenbrock_inp, Active, Duplicated(x, dx))\n((nothing,),)\n\njulia> dx\n2-element Vector{Float64}:\n -400.0\n  200.0","category":"page"},{"location":"","page":"Home","title":"Home","text":"Both the inplace and \"normal\" variant return the gradient. The difference is that with Active the gradient is returned and with Duplicated the gradient is accumulated in place.","category":"page"},{"location":"#Forward-mode","page":"Home","title":"Forward mode","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The return value of forward mode with a Duplicated return is a tuple containing as the first value the primal return value and as the second value the derivative.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In forward mode Duplicated(x, 0.0) is equivalent to Const(x), except that we can perform more optimizations for Const.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Forward, rosenbrock, Duplicated, Const(1.0), Duplicated(3.0, 1.0))\n(400.0, 400.0)\n\njulia> autodiff(Forward, rosenbrock, Duplicated, Duplicated(1.0, 1.0), Const(3.0))\n(400.0, -800.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Of note, when we seed both arguments at once the tangent return is the sum of both.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Forward, rosenbrock, Duplicated, Duplicated(1.0, 1.0), Duplicated(3.0, 1.0))\n(400.0, -400.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can also use forward mode with our inplace method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> x = [1.0, 3.0]\n2-element Vector{Float64}:\n 1.0\n 3.0\n\njulia> dx=[1.0, 1.0]\n2-element Vector{Float64}:\n 1.0\n 1.0\n\njulia> autodiff(Forward, rosenbrock_inp, Duplicated, Duplicated(x, dx))\n(400.0, -400.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note the seeding through dx.","category":"page"},{"location":"#Vector-forward-mode","page":"Home","title":"Vector forward mode","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We can also use vector mode to calculate both derivatives at once.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Forward, rosenbrock, BatchDuplicated, BatchDuplicated(1.0, (1.0, 0.0)), BatchDuplicated(3.0, (0.0, 1.0)))\n(400.0, (var\"1\" = -800.0, var\"2\" = 400.0))\n\njulia> x = [1.0, 3.0]\n2-element Vector{Float64}:\n 1.0\n 3.0\n\njulia> dx_1 = [1.0, 0.0]; dx_2 = [0.0, 1.0];\n\njulia> autodiff(Forward, rosenbrock_inp, BatchDuplicated, BatchDuplicated(x, (dx_1, dx_2)))\n(400.0, (var\"1\" = -800.0, var\"2\" = 400.0))","category":"page"},{"location":"#Caveats-/-Known-issues","page":"Home","title":"Caveats / Known-issues","text":"","category":"section"},{"location":"#Activity-of-temporary-storage","page":"Home","title":"Activity of temporary storage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you have pass any temporary storage which may be involved in an active computation to a function you want to differentiate, you must also pass in a duplicated temporary storage for use in computing the derivatives. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"function f(x, tmp, n)\n   tmp[1] = 1\n   for i in 1:n\n\t   tmp[1] *= x\n   end\n   tmp[1]\nend\n\n# Incorrect [ returns (0.0,) ]\nEnzyme.autodiff(f, Active(1.2), Const(Vector{Float64}(undef, 1)), Const(5))\n\n# Correct [ returns (10.367999999999999,) == 1.2^4 * 5 ]\nEnzyme.autodiff(f, Active(1.2), Duplicated(Vector{Float64}(undef, 1), Vector{Float64}(undef, 1)), Const(5))","category":"page"},{"location":"#CUDA.jl-support","page":"Home","title":"CUDA.jl support","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CUDA.jl is only support on Julia v1.7.0 and onwards. On 1.6 attempting to differentiate CUDA kernel functions, will not use device overloads correctly and thus return fundamentally wrong results.","category":"page"},{"location":"#Sparse-Arrays","page":"Home","title":"Sparse Arrays","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"At the momment there is limited support for sparse linear algebra operations. Sparse arrays may be used, but care must be taken because backing arrays drop zeros in Julia (unless told not to).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SparseArrays\n\na=sparse([2.0])\nf(a)=sum(a)\n\n# Incorrect: SparseMatrixCSC drops explicit zeros\n# returns 1-element SparseVector{Float64, Int64} with 0 stored entries\nda=sparse([0.0])\n\n# Correct: Prevent SparseMatrixCSC from dropping zeros\n# returns 1-element SparseVector{Float64, Int64} with 1 stored entry:\n#  [1]  =  1.0\nda=sparsevec([1],[0.0])\n\nEnzyme.autodiff(Reverse,f,Active,Duplicated(a,da))\n@show da","category":"page"}]
}
