var documenterSearchIndex = {"docs":
[{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"EditURL = \"https://github.com/EnzymeAD/Enzyme.jl/blob/main/examples/autodiff.jl\"","category":"page"},{"location":"generated/autodiff/#AutoDiff-API","page":"AutoDiff API","title":"AutoDiff API","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The goal of this tutorial is to give users already familiar with automatic differentiation (AD) an overview of the Enzyme differentiation API for the following differentiation modes","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Reverse mode\nForward mode\nForward over reverse mode\nVector Forward over reverse mode","category":"page"},{"location":"generated/autodiff/#Defining-a-function","page":"AutoDiff API","title":"Defining a function","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme differentiates arbitrary multivariate vector functions as the most general case in automatic differentiation","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"f mathbbR^n rightarrow mathbbR^m y = f(x)","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"For simplicity we define a vector function with m=1. However, this tutorial can easily be applied to arbitrary m in mathbbN.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"using Enzyme\n\nfunction f(x::Array{Float64}, y::Array{Float64})\n    y[1] = x[1] * x[1] + x[2] * x[1]\n    return nothing\nend;\nnothing #hide","category":"page"},{"location":"generated/autodiff/#Reverse-mode","page":"AutoDiff API","title":"Reverse mode","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The reverse model in AD is defined as","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"beginaligned\ny = f(x) \nbarx = bary cdot nabla f(x)\nendaligned","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"bar denotes an adjoint variable. Note that executing an AD in reverse mode computes both y and the adjoint barx.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"x  = [2.0, 2.0]\nbx = [0.0, 0.0]\ny  = [0.0]\nby = [1.0];\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme stores the value and adjoint of a variable in an object of type Duplicated where the first element represent the value and the second the adjoint. Evaluating the reverse model using Enzyme is done via the following call.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme.autodiff(Reverse, f, Duplicated(x, bx), Duplicated(y, by));\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"This yields the gradient of f in bx at point x = [2.0, 2.0]. by is called the seed and has to be set to 10 in order to compute the gradient. Let's save the gradient for later.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g = copy(bx)","category":"page"},{"location":"generated/autodiff/#Forward-mode","page":"AutoDiff API","title":"Forward mode","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The forward model in AD is defined as","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"beginaligned\ny = f(x) \ndoty = nabla f(x) cdot x\nendaligned","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"To obtain the first element of the gradient using the forward model we have to seed dotx with dotx = 1000","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"x  = [2.0, 2.0]\ndx = [1.0, 0.0]\ny  = [0.0]\ndy = [0.0];\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"In the forward mode the second element of Duplicated stores the tangent.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme.autodiff(Forward, f, Duplicated(x, dx), Duplicated(y, dy));\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"We can now verify that indeed the reverse mode and forward mode yield the same result for the first component of the gradient. Note that to acquire the full gradient one needs to execute the forward model a second time with the seed dx set to [0.0,1.0].","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Let's verify whether the reverse and forward model agree.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g[1] == dy[1]","category":"page"},{"location":"generated/autodiff/#Forward-over-reverse","page":"AutoDiff API","title":"Forward over reverse","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The forward over reverse (FoR) model is obtained by applying the forward model to the reverse model using the chain rule for the product in the adjoint statement.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"beginaligned\ny = f(x) \ndoty = f(x) cdot dotx \nbarx = bary cdot nabla f(x) \ndotbarx = bary cdot nabla^2 f(x) cdot dotx + dotbary cdot nabla f(x)\nendaligned","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"To obtain the first column/row of the Hessian nabla^2 f(x) we have to seed dotbary with 00, bary with 10 and dotx with 10 00.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"y = [0.0]\nx = [2.0, 2.0]\n\ndy = [0.0]\ndx = [1.0, 0.0]\n\nbx = [0.0, 0.0]\nby = [1.0]\ndbx = [0.0, 0.0]\ndby = [0.0]\n\nEnzyme.autodiff(\n    Forward,\n    (x,y) -> Enzyme.autodiff_deferred(Reverse, f, x, y),\n    Duplicated(Duplicated(x, bx), Duplicated(dx, dbx)),\n    Duplicated(Duplicated(y, by), Duplicated(dy, dby)),\n)","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The FoR model also computes the forward model from before, giving us again the first component of the gradient.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g[1] == dy[1]","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"In addition we now have the first row/column of the Hessian.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"dbx[1] == 2.0\ndbx[2] == 1.0","category":"page"},{"location":"generated/autodiff/#Vector-forward-over-reverse","page":"AutoDiff API","title":"Vector forward over reverse","text":"","category":"section"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The vector FoR allows us to propagate several tangents at once through the second-order model. This allows us the acquire the Hessian in one autodiff call. The multiple tangents are organized in tuples. Following the same seeding strategy as before, we now seed both in the vdx[1]=[1.0, 0.0] and vdx[2]=[0.0, 1.0] direction. These tuples have to be put into a BatchDuplicated type.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"y = [0.0]\nx = [2.0, 2.0]\n\nvdy = ([0.0],[0.0])\nvdx = ([1.0, 0.0], [0.0, 1.0])\n\nbx = [0.0, 0.0]\nby = [1.0]\nvdbx = ([0.0, 0.0], [0.0, 0.0])\nvdby = ([0.0], [0.0]);\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"The BatchedDuplicated objects are constructed using the broadcast operator on our tuples of Duplicated for the tangents.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Enzyme.autodiff(\n    Forward,\n    (x,y) -> Enzyme.autodiff_deferred(Reverse, f, x, y),\n    BatchDuplicated(Duplicated(x, bx), Duplicated.(vdx, vdbx)),\n    BatchDuplicated(Duplicated(y, by), Duplicated.(vdy, vdby)),\n);\nnothing #hide","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"Again we obtain the first-order gradient.","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"g[1] == vdy[1][1]","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"We have now the first row/column of the Hessian","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"vdbx[1][1] == 2.0\n\nvdbx[1][2] == 1.0","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"as well as the second row/column","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"vdbx[2][1] == 1.0\n\nvdbx[2][2] == 0.0","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"","category":"page"},{"location":"generated/autodiff/","page":"AutoDiff API","title":"AutoDiff API","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Types-and-constants","page":"API","title":"Types and constants","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [Enzyme, EnzymeCore, EnzymeCore.EnzymeRules]\nOrder = [:type, :constant]","category":"page"},{"location":"api/#Functions-and-macros","page":"API","title":"Functions and macros","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [Enzyme, EnzymeCore, EnzymeCore.EnzymeRules]\nOrder = [:macro, :function]","category":"page"},{"location":"api/#Documentation","page":"API","title":"Documentation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [Enzyme, EnzymeCore, EnzymeCore.EnzymeRules]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"api/#Enzyme.gradient!-Tuple{EnzymeCore.ReverseMode, Any, Any, Any}","page":"API","title":"Enzyme.gradient!","text":"gradient!(::ReverseMode, dx, f, x)\n\nCompute the gradient of an array-input function f using reverse mode, storing the derivative result in an existing array dx.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ndx = [0.0, 0.0]\ngradient!(Reverse, dx, f, [2.0, 3.0])\n\n# output\n\n2-element Vector{Float64}:\n 3.0\n 2.0\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.gradient-Tuple{EnzymeCore.ForwardMode, Any, Any}","page":"API","title":"Enzyme.gradient","text":"gradient(::ForwardMode, f, x; shadow=onehot(x))\n\nCompute the gradient of an array-input function f using forward mode. The optional keyword argument shadow is a vector of one-hot vectors of type x which are used to forward-propagate into the return. For performance reasons, this should be computed once, outside the call to gradient, rather than within this call.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ngrad = gradient(Forward, f, [2.0, 3.0])\n\n# output\n\n(3.0, 2.0)\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.gradient-Tuple{EnzymeCore.ReverseMode, Any, Any}","page":"API","title":"Enzyme.gradient","text":"gradient(::ReverseMode, f, x)\n\nCompute the gradient of an array-input function f using reverse mode. This will allocate and return new array with the gradient result.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ngrad = gradient(Reverse, f, [2.0, 3.0])\n\n# output\n\n2-element Vector{Float64}:\n 3.0\n 2.0\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.gradient-Union{Tuple{chunk}, Tuple{X}, Tuple{F}, Tuple{EnzymeCore.ForwardMode, F, X, Val{chunk}}} where {F, X, chunk}","page":"API","title":"Enzyme.gradient","text":"gradient(::ForwardMode, f, x, ::Val{chunk}; shadow=onehot(x))\n\nCompute the gradient of an array-input function f using vector forward mode. Like gradient, except it uses a chunk size of chunk to compute chunk derivatives in a single call.\n\nExample:\n\nf(x) = x[1]*x[2]\n\ngrad = gradient(Forward, f, [2.0, 3.0], Val(2))\n\n# output\n\n(3.0, 2.0)\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.jacobian-Tuple{EnzymeCore.ForwardMode, Any, Any}","page":"API","title":"Enzyme.jacobian","text":"jacobian(::ForwardMode, f, x; shadow=onehot(x))\njacobian(::ForwardMode, f, x, ::Val{chunk}; shadow=onehot(x))\n\nCompute the jacobian of an array-input function f using (potentially vector) forward mode. This is a simple rename of the gradient function, and all relevant arguments apply here.\n\nExample:\n\nf(x) = [x[1]*x[2], x[2]]\n\ngrad = jacobian(Forward, f, [2.0, 3.0])\n\n# output\n\n2×2 Matrix{Float64}:\n 3.0  2.0\n 0.0  1.0\n\n\n\n\n\n","category":"method"},{"location":"api/#Enzyme.jacobian-Union{Tuple{n_out_val}, Tuple{chunk}, Tuple{X}, Tuple{F}, Tuple{EnzymeCore.ReverseMode, F, X, Val{n_out_val}, Val{chunk}}} where {F, X, chunk, n_out_val}","page":"API","title":"Enzyme.jacobian","text":"jacobian(::ReverseMode, f, x, ::Val{num_outs}, ::Val{chunk})\n\nCompute the jacobian of an array-input function f using (potentially vector) reverse mode. The chunk argument denotes the chunk size to use and num_outs denotes the number of outputs f will return in an array.\n\nExample:\n\nf(x) = [x[1]*x[2], x[2]]\n\ngrad = jacobian(Reverse, f, [2.0, 3.0], Val(2))\n\n# output\n\n2×2 Matrix{Float64}:\n 3.0  2.0\n 0.0  1.0\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff-Union{Tuple{A}, Tuple{FA}, Tuple{EnzymeCore.ForwardMode, FA, Type{A}, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, A<:EnzymeCore.Annotation}","page":"API","title":"EnzymeCore.autodiff","text":"autodiff(::ForwardMode, f, Activity, args...)\n\nAuto-differentiate function f at arguments args using forward mode.\n\nargs may be numbers, arrays, structs of numbers, structs of arrays and so on. Enzyme will only differentiate in respect to arguments that are wrapped in a Duplicated or similar argument. Non-annotated arguments will automatically be treated as Const. Unlike reverse mode in autodiff, Active arguments are not allowed here, since all derivative results of immutable objects will be returned and should instead use Duplicated or variants like DuplicatedNoNeed.\n\nActivity is the Activity of the return value, it may be:\n\nConst if the return is not to be differentiated with respect to\nDuplicated, if the return is being differentiated with respect to and both the original value and the derivative return are desired\nDuplicatedNoNeed, if the return is being differentiated with respect to and only the derivative return is desired.\nBatchDuplicated, like Duplicated, but computing multiple derivatives at once. All batch sizes must be the same for all arguments.\nBatchDuplicatedNoNeed, like DuplicatedNoNeed, but computing multiple derivatives at one. All batch sizes must be the same for all arguments.\n\nExample returning both original return and derivative:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(x) = x*x\nres, ∂f_∂x = autodiff(Forward, f, Duplicated, Duplicated(3.14, 1.0))\n\n# output\n\n(9.8596, 6.28)\n\nExample returning just the derivative:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(x) = x*x\n∂f_∂x = autodiff(Forward, f, DuplicatedNoNeed, Duplicated(3.14, 1.0))\n\n# output\n\n(6.28,)\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff-Union{Tuple{CMode}, Tuple{FA}, Tuple{CMode, FA, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, CMode<:EnzymeCore.Mode}","page":"API","title":"EnzymeCore.autodiff","text":"autodiff(mode::Mode, f, args...)\n\nLike autodiff but will try to guess the activity of the return value.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff-Union{Tuple{CMode}, Tuple{F}, Tuple{CMode, F, Vararg{Any}}} where {F, CMode<:EnzymeCore.Mode}","page":"API","title":"EnzymeCore.autodiff","text":"autodiff(mode::Mode, f, ::Type{A}, args...)\n\nLike autodiff but will try to extend f to an annotation, if needed.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff-Union{Tuple{ReturnPrimal}, Tuple{A}, Tuple{FA}, Tuple{EnzymeCore.ReverseMode{ReturnPrimal}, FA, Type{A}, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, A<:EnzymeCore.Annotation, ReturnPrimal}","page":"API","title":"EnzymeCore.autodiff","text":"autodiff(::ReverseMode, f, Activity, args...)\n\nAuto-differentiate function f at arguments args using reverse mode.\n\nLimitations:\n\nf may only return a Real (of a built-in/primitive type) or nothing, not an array, struct, BigFloat, etc. To handle vector-valued return types, use a mutating f! that returns nothing and stores it's return value in one of the arguments, which must be wrapped in a Duplicated.\n\nargs may be numbers, arrays, structs of numbers, structs of arrays and so on. Enzyme will only differentiate in respect to arguments that are wrapped in an Active (for arguments whose derivative result must be returned rather than mutated in place, such as primitive types and structs thereof) or Duplicated (for mutable arguments like arrays, Refs and structs thereof). Non-annotated arguments will automatically be treated as Const.\n\nActivity is the Activity of the return value, it may be Const or Active.\n\nExample:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(a, b, c, d) = a * √(b[1]^2 + b[2]^2) + c^2 * d^2\n∂f_∂a, _, _, ∂f_∂d = autodiff(Reverse, f, Active, Active(a), Duplicated(b, ∂f_∂b), c, Active(d))[1]\n\n# output\n\n(3.966106403010388, nothing, nothing, 54450.0)\n\nhere, autodiff returns a tuple (partial fpartial a partial fpartial d), while partial fpartial b will be added to ∂f_∂b (but not returned). c will be treated as Const(c).\n\nOne can also request the original returned value of the computation.\n\nExample:\n\nEnzyme.autodiff(ReverseWithPrimal, x->x*x, Active(3.0))\n\n# output\n\n((6.0,), 9.0)\n\nnote: Note\nEnzyme gradients with respect to integer values are zero. Active will automatically convert plain integers to floating point values, but cannot do so for integer values in tuples and structs.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_deferred-Union{Tuple{A}, Tuple{FA}, Tuple{EnzymeCore.ForwardMode, FA, Type{A}, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, A<:EnzymeCore.Annotation}","page":"API","title":"EnzymeCore.autodiff_deferred","text":"autodiff_deferred(::ForwardMode, f, Activity, args...)\n\nSame as autodiff(::ForwardMode, ...) but uses deferred compilation to support usage in GPU code, as well as high-order differentiation.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_deferred-Union{Tuple{CMode}, Tuple{F}, Tuple{CMode, F, Vararg{Any}}} where {F, CMode<:EnzymeCore.Mode}","page":"API","title":"EnzymeCore.autodiff_deferred","text":"autodiff_deferred(mode::Mode, f, ::Type{A}, args...)\n\nLike autodiff_deferred but will try to extend f to an annotation, if needed.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_deferred-Union{Tuple{ReturnPrimal}, Tuple{A}, Tuple{FA}, Tuple{EnzymeCore.ReverseMode{ReturnPrimal}, FA, Type{A}, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, A<:EnzymeCore.Annotation, ReturnPrimal}","page":"API","title":"EnzymeCore.autodiff_deferred","text":"autodiff_deferred(::ReverseMode, f, Activity, args...)\n\nSame as autodiff but uses deferred compilation to support usage in GPU code, as well as high-order differentiation.\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_deferred_thunk-Union{Tuple{ModifiedBetweenT}, Tuple{Width}, Tuple{ReturnShadow}, Tuple{ReturnPrimal}, Tuple{A}, Tuple{FA}, Tuple{EnzymeCore.ReverseModeSplit{ReturnPrimal, ReturnShadow, Width, ModifiedBetweenT}, Type{FA}, Type{A}, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, A<:EnzymeCore.Annotation, ReturnPrimal, ReturnShadow, Width, ModifiedBetweenT}","page":"API","title":"EnzymeCore.autodiff_deferred_thunk","text":"autodiff_deferred_thunk(::ReverseModeSplit, ftype, Activity, argtypes...)\n\nProvide the split forward and reverse pass functions for annotated function type ftype when called with args of type argtypes when using reverse mode.\n\nActivity is the Activity of the return value, it may be Const, Active, or Duplicated (or its variants DuplicatedNoNeed, BatchDuplicated, and BatchDuplicatedNoNeed).\n\nThe forward function will return a tape, the primal (or nothing if not requested), and the shadow (or nothing if not a Duplicated variant), and tapes the corresponding type arguements provided.\n\nThe reverse function will return the derivative of Active arguments, updating the Duplicated arguments in place. The same arguments to the forward pass should be provided, followed by the adjoint of the return (if the return is active), and finally the tape from the forward pass.\n\nExample:\n\n\nA = [2.2]; ∂A = zero(A)\nv = 3.3\n\nfunction f(A, v)\n    res = A[1] * v\n    A[1] = 0\n    res\nend\n\nforward, reverse = autodiff_deferred_thunk(ReverseSplitWithPrimal, Const{typeof(f)}, Active, Duplicated{typeof(A)}, Active{typeof(v)})\n\ntape, result, shadow_result  = forward(Const(f), Duplicated(A, ∂A), Active(v))\n_, ∂v = reverse(Const(f), Duplicated(A, ∂A), Active(v), 1.0, tape)[1]\n\nresult, ∂v, ∂A \n\n# output\n\n(7.26, 2.2, [3.3])\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_thunk-Union{Tuple{A}, Tuple{FA}, Tuple{EnzymeCore.ForwardMode, Type{FA}, Type{A}, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, A<:EnzymeCore.Annotation}","page":"API","title":"EnzymeCore.autodiff_thunk","text":"autodiff_thunk(::ForwardMode, ftype, Activity, argtypes...)\n\nProvide the thunk forward mode function for annotated function type ftype when called with args of type argtypes.\n\nActivity is the Activity of the return value, it may be Const or Duplicated (or its variants DuplicatedNoNeed, BatchDuplicated, andBatchDuplicatedNoNeed).\n\nThe forward function will return the primal (if requested) and the shadow (or nothing if not a Duplicated variant).\n\nExample returning both original return and derivative:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(x) = x*x\nforward = autodiff_thunk(Forward, Const{typeof(f)}, Duplicated, Duplicated{Float64})\nres, ∂f_∂x = forward(Const(f), Duplicated(3.14, 1.0))\n\n# output\n\n(9.8596, 6.28)\n\nExample returning just the derivative:\n\na = 4.2\nb = [2.2, 3.3]; ∂f_∂b = zero(b)\nc = 55; d = 9\n\nf(x) = x*x\nforward = autodiff_thunk(Forward, Const{typeof(f)}, DuplicatedNoNeed, Duplicated{Float64})\n∂f_∂x = forward(Const(f), Duplicated(3.14, 1.0))\n\n# output\n\n(6.28,)\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.autodiff_thunk-Union{Tuple{ModifiedBetweenT}, Tuple{Width}, Tuple{ReturnShadow}, Tuple{ReturnPrimal}, Tuple{A}, Tuple{FA}, Tuple{EnzymeCore.ReverseModeSplit{ReturnPrimal, ReturnShadow, Width, ModifiedBetweenT}, Type{FA}, Type{A}, Vararg{Any}}} where {FA<:EnzymeCore.Annotation, A<:EnzymeCore.Annotation, ReturnPrimal, ReturnShadow, Width, ModifiedBetweenT}","page":"API","title":"EnzymeCore.autodiff_thunk","text":"autodiff_thunk(::ReverseModeSplit, ftype, Activity, argtypes...)\n\nProvide the split forward and reverse pass functions for annotated function type ftype when called with args of type argtypes when using reverse mode.\n\nActivity is the Activity of the return value, it may be Const, Active, or Duplicated (or its variants DuplicatedNoNeed, BatchDuplicated, and BatchDuplicatedNoNeed).\n\nThe forward function will return a tape, the primal (or nothing if not requested), and the shadow (or nothing if not a Duplicated variant), and tapes the corresponding type arguements provided.\n\nThe reverse function will return the derivative of Active arguments, updating the Duplicated arguments in place. The same arguments to the forward pass should be provided, followed by the adjoint of the return (if the return is active), and finally the tape from the forward pass.\n\nExample:\n\n\nA = [2.2]; ∂A = zero(A)\nv = 3.3\n\nfunction f(A, v)\n    res = A[1] * v\n    A[1] = 0\n    res\nend\n\nforward, reverse = autodiff_thunk(ReverseSplitWithPrimal, Const{typeof(f)}, Active, Duplicated{typeof(A)}, Active{typeof(v)})\n\ntape, result, shadow_result  = forward(Const(f), Duplicated(A, ∂A), Active(v))\n_, ∂v = reverse(Const(f), Duplicated(A, ∂A), Active(v), 1.0, tape)[1]\n\nresult, ∂v, ∂A \n\n# output\n\n(7.26, 2.2, [3.3])\n\n\n\n\n\n","category":"method"},{"location":"api/#EnzymeCore.Active","page":"API","title":"EnzymeCore.Active","text":"Active(x)\n\nMark a function argument x of autodiff as active, Enzyme will auto-differentiate in respect Active arguments.\n\nnote: Note\nEnzyme gradients with respect to integer values are zero. Active will automatically convert plain integers to floating point values, but cannot do so for integer values in tuples and structs.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Annotation","page":"API","title":"EnzymeCore.Annotation","text":"abstract type Annotation{T}\n\nAbstract type for autodiff function argument wrappers like Const, Active and Duplicated.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.BatchDuplicated","page":"API","title":"EnzymeCore.BatchDuplicated","text":"BatchDuplicated(x, ∂f_∂xs)\n\nLike Duplicated, except contains several shadows to compute derivatives for all at once. Argument ∂f_∂xs should be a tuple of the several values of type x.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.BatchDuplicatedNoNeed","page":"API","title":"EnzymeCore.BatchDuplicatedNoNeed","text":"BatchDuplicatedNoNeed(x, ∂f_∂xs)\n\nLike DuplicatedNoNeed, except contains several shadows to compute derivatives for all at once. Argument ∂f_∂xs should be a tuple of the several values of type x.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Const","page":"API","title":"EnzymeCore.Const","text":"Const(x)\n\nMark a function argument x of autodiff as constant, Enzyme will not auto-differentiate in respect Const arguments.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Duplicated","page":"API","title":"EnzymeCore.Duplicated","text":"Duplicated(x, ∂f_∂x)\n\nMark a function argument x of autodiff as duplicated, Enzyme will auto-differentiate in respect to such arguments, with dx acting as an accumulator for gradients (so partial f  partial x will be added to) ∂f_∂x.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.DuplicatedNoNeed","page":"API","title":"EnzymeCore.DuplicatedNoNeed","text":"DuplicatedNoNeed(x, ∂f_∂x)\n\nLike Duplicated, except also specifies that Enzyme may avoid computing the original result and only compute the derivative values.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.ForwardMode","page":"API","title":"EnzymeCore.ForwardMode","text":"struct Forward <: Mode\n\nForward mode differentiation\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.Mode","page":"API","title":"EnzymeCore.Mode","text":"abstract type Mode\n\nAbstract type for what differentiation mode will be used.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.ReverseMode","page":"API","title":"EnzymeCore.ReverseMode","text":"struct ReverseMode{ReturnPrimal} <: Mode\n\nReverse mode differentiation.\n\nReturnPrimal: Should Enzyme return the primal return value from the augmented-forward.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.ReverseModeSplit","page":"API","title":"EnzymeCore.ReverseModeSplit","text":"struct ReverseModeSplit{ReturnPrimal,ReturnShadow,Width,ModifiedBetween} <: Mode\n\nReverse mode differentiation.\n\nReturnPrimal: Should Enzyme return the primal return value from the augmented-forward.\nReturnShadow: Should Enzyme return the shadow return value from the augmented-forward.\nWidth: Batch Size (0 if to be automatically derived)\nModifiedBetween: Tuple of each argument's modified between state (true if to be automatically derived).\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.EnzymeRules.AugmentedReturn","page":"API","title":"EnzymeCore.EnzymeRules.AugmentedReturn","text":"AugmentedReturn(primal, shadow, tape)\n\nAugment the primal return value of a function with its shadow, as well as any additional information needed to correctly  compute the reverse pass, stored in tape.\n\nUnless specified by the config that a variable is not overwritten, rules must assume any arrays/data structures/etc are  overwritten between the forward and the reverse pass. Any floats or variables passed by value are always preserved as is  (as are the arrays themselves, just not necessarily the values in the array).\n\nSee also augmented_primal.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.EnzymeRules.Config","page":"API","title":"EnzymeCore.EnzymeRules.Config","text":"Config{NeedsPrimal, NeedsShadow, Width, Overwritten}\nConfigWidth{Width} = Config{<:Any,<:Any, Width}\n\nConfiguration type to dispatch on in custom reverse rules (see augmented_primal and reverse).\n\nNeedsPrimal and NeedsShadow: boolean values specifying whether the primal and shadow (resp.) should be returned. \nWidth: an integer that specifies the number of adjoints/shadows simultaneously being propagated.\nOverwritten: a tuple of booleans of whether each argument (including the function itself) is modified between the   forward and reverse pass (true if potentially modified between).\n\nGetters for the four type parameters are provided by needs_primal, needs_shadow, width, and overwritten.\n\n\n\n\n\n","category":"type"},{"location":"api/#EnzymeCore.EnzymeRules.augmented_primal","page":"API","title":"EnzymeCore.EnzymeRules.augmented_primal","text":"augmented_primal(::Config, func::Annotation{typeof(f)}, RT::Type{<:Annotation}, args::Annotation...)\n\nMust return an AugmentedReturn type.\n\nThe primal must be the same type of the original return if needs_primal(config), otherwise nothing.\nThe shadow must be nothing if needs_shadow(config) is false. If width is 1, the shadow should be the same type of the original return. If the width is greater than 1, the shadow should be NTuple{original return, width}.\nThe tape can be any type (including Nothing) and is preserved for the reverse call.\n\n\n\n\n\n","category":"function"},{"location":"api/#EnzymeCore.EnzymeRules.forward","page":"API","title":"EnzymeCore.EnzymeRules.forward","text":"forward(func::Annotation{typeof(f)}, RT::Type{<:Annotation}, args::Annotation...)\n\nCalculate the forward derivative. The first argument func is the callable for which the rule applies to. Either wrapped in a Const), or a Duplicated if it is a closure. The second argument is the return type annotation, and all other arguments are the annotated function arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#EnzymeCore.EnzymeRules.inactive","page":"API","title":"EnzymeCore.EnzymeRules.inactive","text":"inactive(func::typeof(f), args...)\n\nMark a particular function as always being inactive in both its return result and the function call itself.\n\n\n\n\n\n","category":"function"},{"location":"api/#EnzymeCore.EnzymeRules.reverse","page":"API","title":"EnzymeCore.EnzymeRules.reverse","text":"reverse(::Config, func::Annotation{typeof(f)}, dret::Active, tape, args::Annotation...)\nreverse(::Config, func::Annotation{typeof(f)}, ::Type{<:Annotation), tape, args::Annotation...)\n\nTakes gradient of derivative, activity annotation, and tape. If there is an active return dret is passed as Active{T} with the derivative of the active return val. Otherwise dret is passed as Type{Duplicated{T}}, etc.\n\n\n\n\n\n","category":"function"},{"location":"pullbacks/#Implementing-pullbacks","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"","category":"section"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Enzyme's autodiff function can only handle functions with scalar output. To implement pullbacks (back-propagation of gradients/tangents) for array-valued functions, use a mutating function that returns nothing and stores it's result in one of the arguments, which must be passed wrapped in a Duplicated.","category":"page"},{"location":"pullbacks/#Example","page":"Implementing pullbacks","title":"Example","text":"","category":"section"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Given a function mymul! that performs the equivalent of R = A * B for matrices A and B, and given a gradient (tangent) ∂z_∂R, we can compute ∂z_∂A and ∂z_∂B like this:","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"using Enzyme, Random\n\nfunction mymul!(R, A, B)\n    @assert axes(A,2) == axes(B,1)\n    @inbounds @simd for i in eachindex(R)\n        R[i] = 0\n    end\n    @inbounds for j in axes(B, 2), i in axes(A, 1)\n        @inbounds @simd for k in axes(A,2)\n            R[i,j] += A[i,k] * B[k,j]\n        end\n    end\n    nothing\nend\n\nRandom.seed!(1234)\nA = rand(5, 3)\nB = rand(3, 7)\n\nR = zeros(size(A,1), size(B,2))\n∂z_∂R = rand(size(R)...)  # Some gradient/tangent passed to us\n∂z_∂R0 = copyto!(similar(∂z_∂R), ∂z_∂R)  # exact copy for comparison\n\n∂z_∂A = zero(A)\n∂z_∂B = zero(B)\n\nEnzyme.autodiff(Reverse, mymul!, Const, Duplicated(R, ∂z_∂R), Duplicated(A, ∂z_∂A), Duplicated(B, ∂z_∂B))","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Now we have:","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"R ≈ A * B            &&\n∂z_∂A ≈ ∂z_∂R0 * B'   &&  # equivalent to Zygote.pullback(*, A, B)[2](∂z_∂R)[1]\n∂z_∂B ≈ A' * ∂z_∂R0       # equivalent to Zygote.pullback(*, A, B)[2](∂z_∂R)[2]","category":"page"},{"location":"pullbacks/","page":"Implementing pullbacks","title":"Implementing pullbacks","text":"Note that the result of the backpropagation is added to ∂z_∂A and ∂z_∂B, they act as accumulators for gradient information.","category":"page"},{"location":"dev_docs/#Enzyme-developer-documentation","page":"For developers","title":"Enzyme developer documentation","text":"","category":"section"},{"location":"dev_docs/#Development-of-Enzyme-and-Enzyme.jl-together","page":"For developers","title":"Development of Enzyme and Enzyme.jl together","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Normally Enzyme.jl downloads and install Enzyme for the user automatically since Enzyme needs to be built against Julia bundeled LLVM. In case that you are making updates to Enzyme and want to test them against Enzyme.jl the instructions below should help you get started.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Start Julia in your development copy of Enzyme.jl","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"~/s/Enzyme (master)> julia --project=.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Then create a development copy of Enzyme_jll and activate it within.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"julia> using Enzyme_jll\njulia> Enzyme_jll.dev_jll()\n[ Info: Enzyme_jll dev'ed out to ${JULIA_PKG_DEVDIR}/Enzyme_jll with pre-populated override directory\n(Enzyme) pkg> dev Enzyme_jll\nPath `${JULIA_PKG_DEVDIR}/Enzyme_jll` exists and looks like the correct package. Using existing path.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"After restarting Julia:","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"julia> Enzyme_jll.dev_jll()\njulia> Enzyme_jll.libEnzyme_path\n\"${JULIA_PKG_DEVDIR}/Enzyme_jll/override/lib/LLVMEnzyme-9.so\"","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"On your machine ${JULIA_PKG_DEVDIR} most likely corresponds to ~/.julia/dev. Now we can inspect \"${JULIA_PKG_DEVDIR}/Enzyme_jll/override/lib and see that there is a copy of LLVMEnzyme-9.so, which we can replace with a symbolic link or a copy of a version of Enzyme.","category":"page"},{"location":"dev_docs/#Building-Enzyme-against-Julia's-LLVM.","page":"For developers","title":"Building Enzyme against Julia's LLVM.","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Depending on how you installed Julia the LLVM Julia is using will be different.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"Download from julialang.org (Recommended)\nManual build on your machine\nUses a pre-built Julia from your system vendor (Not recommended)","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"To check what LLVM Julia is using use:","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"julia> Base.libllvm_version_string\n\"9.0.1jl\"","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"If the LLVM version ends in a jl you a likely using the private LLVM.","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"In your source checkout of Enzyme:","category":"page"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"mkdir build-jl\ncd build-jl","category":"page"},{"location":"dev_docs/#Prebuilt-binary-from-julialang.org","page":"For developers","title":"Prebuilt binary from julialang.org","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"LLVM_MAJOR_VER=`julia -e \"print(Base.libllvm_version.major)\"`\njulia -e \"using Pkg; pkg\\\"add LLVM_full_jll@${LLVM_MAJOR_VER}\\\"\"\nLLVM_DIR=`julia -e \"using LLVM_full_jll; print(LLVM_full_jll.artifact_dir)\"`\necho \"LLVM_DIR=$LLVM_DIR\"\ncmake ../enzyme/ -G Ninja -DENZYME_EXTERNAL_SHARED_LIB=ON -DLLVM_DIR=${LLVM_DIR} -DLLVM_EXTERNAL_LIT=${LLVM_DIR}/tools/lit/lit.py","category":"page"},{"location":"dev_docs/#Manual-build-of-Julia","page":"For developers","title":"Manual build of Julia","text":"","category":"section"},{"location":"dev_docs/","page":"For developers","title":"For developers","text":"cmake ../enzyme/ -G Ninja -DENZYME_EXTERNAL_SHARED_LIB=ON -DLLVM_DIR=${PATH_TO_BUILDDIR_OF_JULIA}/usr/lib/cmake/llvm/","category":"page"},{"location":"internal_api/#Internal-API","page":"Internal API","title":"Internal API","text":"","category":"section"},{"location":"internal_api/","page":"Internal API","title":"Internal API","text":"note: Note\nThis is the documentation of Enzymes's internal API. The internal API is not subject to semantic versioning and may change at any time and without deprecation.","category":"page"},{"location":"internal_api/","page":"Internal API","title":"Internal API","text":"Modules = [Enzyme.Compiler]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"internal_api/#Enzyme.Compiler.fspec-Tuple{Any, Any, Integer}","page":"Internal API","title":"Enzyme.Compiler.fspec","text":"Create the methodinstance pair, and lookup the primal return type.\n\n\n\n\n\n","category":"method"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"EditURL = \"https://github.com/EnzymeAD/Enzyme.jl/blob/main/examples/box.jl\"","category":"page"},{"location":"generated/box/#Enzyme-for-adjoint-tutorial:-Stommel-three-box-ocean-model","page":"Box model","title":"Enzyme for adjoint tutorial: Stommel three-box ocean model","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"The goal of this tutorial is to teach about a specific usage of Enzyme's automatic differentiation capabilities, and will be centered around the Stommel ocean model. This is a nice example to see how powerful Enzyme is, and the ability of it to take a derivative of a complicated function (namely one that has many parts and parameters). This tutorial will focus first on the computations and getting Enzyme running, for those interested a mathematical explanation of the model and what an adjoint variable is will be provided at the end.","category":"page"},{"location":"generated/box/#Brief-model-overview","page":"Box model","title":"Brief model overview","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"The Stommel box model can be viewed as a watered down full ocean model. In our example, we have three boxes (Box One, Box Two, and Box Three) and we model the transport of fluid between them. The full equations of our system are given by:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"beginaligned\n   U = u_0 left rho_2 - left rho_1 + (1 - delta) rho_3 right right \n   rho_i = -alpha T_i + beta S_i     i = 1 2 3\nendaligned","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"for the transport U and densities rho, and then the time derivatives","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"beginaligned\n   dotT_1 = U(T_3 - T_1)V_1 + gamma (T_1^* - T_1 )  dotS_1 = U(S_3 - S_1)V_1 + FW_1V_1 \n   dotT_2 = U(T_1 - T_2)V_2 + gamma (T_2^* - T_2 )  dotS_2 = U(S_1 - S_2)V_2 + FW_2V_2 \n   dotT_3 = U(T_2 - T_3)V_3  dotS_3 = U(S_2 - S_3)V_3\nendaligned","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"for positive transport, U  0, and","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"beginaligned\n   dotT_1 = U(T_2 - T_1)V_1 + gamma (T_1^* - T_1)  dotS_1 = U(S_2 - S_1)V_1 + FW_1V_1 \n   dotT_2 = U(T_3 - T_2)V_2 + gamma (T_2^* - T_2 )  dotS_2 = U(S_3 - S_2)V_2 + FW_2V_2 \n   dotT_3 = U(T_1 - T_3)V_3  dotS_3 = U(S_1 - S_3)V_3\nendaligned","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"for U leq 0. The only force driving our system is a density gradient generated via temperature and salinity differences between the boxes. This makes it a really easy model to play around with! With this in mind, the model is run forward with the steps:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Compute densities\nCompute transport\nCompute time derivatives of the box temperatures and salinities\nUpdate the state vector","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"We'll start by going through the model setup step by step, then providing a few test cases with Enzyme.","category":"page"},{"location":"generated/box/#Model-setup","page":"Box model","title":"Model setup","text":"","category":"section"},{"location":"generated/box/#Model-dependencies","page":"Box model","title":"Model dependencies","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Let's first add the necessary packages to run everything","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"using Enzyme","category":"page"},{"location":"generated/box/#Initialize-constants","page":"Box model","title":"Initialize constants","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"The system equations have quite a few constants that appear, here we initialize them for later use. We'll do this in a Julia way: we have an empty structure that will hold all the parameters, and a function (we'll call this setup) that initializes them. This means that, so long as we don't need to change parameters, we only need to run setup once.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"struct ModelParameters\n\n    # handy to have constants\n    day::Float64\n    year::Float64\n\n    # Information related to the boxes\n    boxlength::Vector{Float64}      ## Vector with north-south size of each box  [cm]\n    boxdepth::Vector{Float64}       ## \"          \" the depth of each box  [cm]\n    boxwidth::Float64               ## \"          \" the width of each box  [cm]\n    boxarea::Vector{Float64}        ## \"          \" the area of each box   [cm^2]\n    boxvol::Vector{Float64}         ## \"          \" the volume of each box   [cm^3]\n\n    delta::Float64                  ## Constant ratio depth(box1) / (depth(box1) + depth(box3))\n\n    # Parameters that appear in the box model equations\n    u0::Float64\n    alpha::Float64\n    beta::Float64\n    gamma::Float64\n\n    # Coefficient for the Robert filter smoother\n    rf_coeff::Float64\n\n    # Freshwater forcing\n    FW::Vector{Float64}\n\n    # Restoring atmospheric temperatures and salinities\n    Tstar::Vector{Float64}\n    Sstar::Vector{Float64}\n\nend\n\nfunction setup()\n\n    blength = [5000.0e5; 1000.0e5; 5000.0e5]\n    bdepth = [1.0e5; 5.0e5; 4.0e5]\n\n    delta = bdepth[1]/(bdepth[1] + bdepth[3])\n\n    bwidth = 4000.0*1e5  ## box width, centimeters\n\n    # box areas\n    barea = [blength[1]*bwidth;\n            blength[2]*bwidth;\n            blength[3]*bwidth]\n\n    # box volumes\n    bvolume = [barea[1]*bdepth[1];\n            barea[2]*bdepth[2];\n            barea[3]*bdepth[3]]\n\n    # parameters that are used to ensure units are in CGS (cent-gram-sec)\n\n    day = 3600.0*24.0\n    year = day*365.0\n    Sv = 1e12                       ## one Sverdrup (a unit of ocean transport), 1e6 meters^3/second\n\n    # parameters that appear in box model equations\n    u0 = 16.0*Sv/0.0004\n    alpha = 1668e-7\n    beta = 0.7811e-3\n\n    gamma = 1/(300*day)\n\n    # robert filter coefficient for the smoother part of the timestep\n    robert_filter_coeff = 0.25\n\n    # freshwater forcing\n    FW = [(100/year) * 35.0 * barea[1]; -(100/year) * 35.0 * barea[1]]\n\n    # restoring atmospheric temperatures\n    Tstar = [22.0; 0.0]\n    Sstar = [36.0; 34.0]\n\n    structure_with_parameters = ModelParameters(day,\n        year,\n        blength,\n        bdepth,\n        bwidth,\n        barea,\n        bvolume,\n        delta,\n        u0,\n        alpha,\n        beta,\n        gamma,\n        robert_filter_coeff,\n        FW,\n        Tstar,\n        Sstar\n    )\n\n    return structure_with_parameters\n\nend","category":"page"},{"location":"generated/box/#Define-model-functions","page":"Box model","title":"Define model functions","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Here we define functions that will calculate quantities used in the forward steps.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"# function to compute transport\n#       Input: rho - the density vector\n#       Output: U - transport value\n\nfunction compute_transport(rho, params)\n\n    U = params.u0 * (rho[2] - (params.delta * rho[1] + (1 - params.delta)*rho[3]))\n    return U\n\nend\n\n# function to compute density\n#       Input: state = [T1; T2; T3; S1; S2; S3]\n#       Output: rho\n\nfunction compute_density(state, params)\n\n    rho = -params.alpha * state[1:3] + params.beta * state[4:6]\n    return rho\n\nend\n\n# lastly, a function that takes one step forward\n#       Input: state_now = [T1(t), T2(t), ..., S3(t)]\n#              state_old = [T1(t-dt), ..., S3(t-dt)]\n#              u = transport(t)\n#              dt = time step\n#       Output: state_new = [T1(t+dt), ..., S3(t+dt)]\n\nfunction compute_update(state_now, state_old, u, params, dt)\n\n    dstate_now_dt = zeros(6)\n    state_new = zeros(6)\n\n    # first computing the time derivatives of the various temperatures and salinities\n    if u > 0\n\n        dstate_now_dt[1] = u * (state_now[3] - state_now[1]) / params.boxvol[1] + params.gamma * (params.Tstar[1] - state_now[1])\n        dstate_now_dt[2] = u * (state_now[1] - state_now[2]) / params.boxvol[2] + params.gamma * (params.Tstar[2] - state_now[2])\n        dstate_now_dt[3] = u * (state_now[2] - state_now[3]) / params.boxvol[3]\n\n        dstate_now_dt[4] = u * (state_now[6] - state_now[4]) / params.boxvol[1] + params.FW[1] / params.boxvol[1]\n        dstate_now_dt[5] = u * (state_now[4] - state_now[5]) / params.boxvol[2] + params.FW[2] / params.boxvol[2]\n        dstate_now_dt[6] = u * (state_now[5] - state_now[6]) / params.boxvol[3]\n\n    elseif u <= 0\n\n        dstate_now_dt[1] = u * (state_now[2] - state_now[1]) / params.boxvol[1] + params.gamma * (params.Tstar[1] - state_now[1])\n        dstate_now_dt[2] = u * (state_now[3] - state_now[2]) / params.boxvol[2] + params.gamma * (params.Tstar[2] - state_now[2])\n        dstate_now_dt[3] = u * (state_now[1] - state_now[3]) / params.boxvol[3]\n\n        dstate_now_dt[4] = u * (state_now[5] - state_now[4]) / params.boxvol[1] + params.FW[1] / params.boxvol[1]\n        dstate_now_dt[5] = u * (state_now[6] - state_now[5]) / params.boxvol[2] + params.FW[2] / params.boxvol[2]\n        dstate_now_dt[6] = u * (state_now[4] - state_now[6]) / params.boxvol[3]\n\n    end\n\n    # update fldnew using a version of Euler's method\n    state_new .= state_old + 2.0 * dt * dstate_now_dt\n\n    return state_new\nend","category":"page"},{"location":"generated/box/#Define-forward-functions","page":"Box model","title":"Define forward functions","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Finally, we create two functions, the first of which computes and stores all the states of the system, and the second will take just a single step forward.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Let's start with the standard forward function. This is just going to be used to store the states at every timestep:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"function integrate(state_now, state_old, dt, M, parameters)\n\n    # Because of the adjoint problem we're setting up, we need to store both the states before\n    # and after the Robert filter smoother has been applied\n    states_before = [state_old]\n    states_after = [state_old]\n\n    for t = 1:M\n\n        rho = compute_density(state_now, parameters)\n        u = compute_transport(rho, parameters)\n        state_new = compute_update(state_now, state_old, u, parameters, dt)\n\n        # Applying the Robert filter smoother (needed for stability)\n        state_new_smoothed = state_now + parameters.rf_coeff * (state_new - 2.0 * state_now + state_old)\n\n        push!(states_after, state_new_smoothed)\n        push!(states_before, state_new)\n\n        # cycle the \"now, new, old\" states\n        state_old = state_new_smoothed\n        state_now = state_new\n\n    end\n\n    return states_after, states_before\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Now, for the purposes of Enzyme, it would be convenient for us to have a function that runs a single step of the model forward rather than the whole integration. This would allow us to save as many of the adjoint variables as we wish when running the adjoint method, although for the example we'll discuss later we technically only need one of them","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"function one_step_forward(state_now, state_old, out_now, out_old, parameters, dt)\n\n    state_new_smoothed = zeros(6)\n    rho = compute_density(state_now, parameters)                             ## compute density\n    u = compute_transport(rho, parameters)                                   ## compute transport\n    state_new = compute_update(state_now, state_old, u, parameters, dt)      ## compute new state values\n\n    # Robert filter smoother\n    state_new_smoothed[:] = state_now + parameters.rf_coeff * (state_new - 2.0 * state_now + state_old)\n\n    out_old[:] = state_new_smoothed\n    out_now[:] = state_new\n\n    return nothing\n\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"One difference to note is thatone_step_forward now returns nothing, but is rather a function of both its input and output. Since the output of the function is a vector, we need to have this return nothing for Enzyme to work. Now we can move on to some examples using Enzyme.","category":"page"},{"location":"generated/box/#Example-1:-Simply-using-Enzyme","page":"Box model","title":"Example 1: Simply using Enzyme","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"For the first example let's just compute the gradient of our forward function and examine the output. We'll just run the model for one step, and take a dt of ten days. The initial conditions of the system are given as Tbar and Sbar. We run setup once here, and never have to run it again! (Unless we decide to change a parameter)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"parameters = setup()\n\nTbar = [20.0; 1.0; 1.0]         ## initial temperatures\nSbar = [35.5; 34.5; 34.5]       ## initial salinities\n\n# Running the model one step forward\nstates_after_smoother, states_before_smoother = integrate(\n    copy([Tbar; Sbar]),\n    copy([Tbar; Sbar]),\n    10*parameters.day,\n    1,\n    parameters\n)\n\n# Run Enzyme one time on `one_step_forward``\ndstate_now = zeros(6)\ndstate_old = zeros(6)\nout_now = zeros(6); dout_now = ones(6)\nout_old = zeros(6); dout_old = ones(6)\n\nautodiff(Reverse,\n    one_step_forward,\n    Duplicated([Tbar; Sbar], dstate_now),\n    Duplicated([Tbar; Sbar], dstate_old),\n    Duplicated(out_now, dout_now),\n    Duplicated(out_old, dout_old),\n    parameters,\n    Const(10*parameters.day)\n)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"In order to run Enzyme on one_step_forward, we've needed to provide quite a few placeholders, and wrap everything in Duplicated as all components of our function are vectors, not scalars. Let's go through and see what Enzyme did with all of those placeholders.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"First we can look at what happened to the zero vectors outnow and outold:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show out_now, out_old","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Comparing to the results of forward func:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show states_before_smoother[2], states_after_smoother[2]","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"we see that Enzyme has computed and stored exactly the output of the forward step. Next, let's look at dstate_now:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show dstate_now","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Just a few numbers, but this is what makes AD so nice: Enzyme has exactly computed the derivative of all outputs with respect to the input innow, evaluated at innow, and acted with this gradient on what we gave as dout_now (in our case, all ones). In math language, this is just","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"textdstate now = (fracpartial textout now(textstate now)partial textstate now + fracpartial textout old(textstate now)partial textstate now) textdout now","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"We note here that had we initialized dstate_now and dstate_old as something else, our results will change. Let's multiply them by two and see what happens.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"dstate_now_new = zeros(6)\ndstate_old_new = zeros(6)\nout_now = zeros(6); dout_now = 2*ones(6)\nout_old = zeros(6); dout_old = 2*ones(6)\nautodiff(Reverse,\n    one_step_forward,\n    Duplicated([Tbar; Sbar], dstate_now_new),\n    Duplicated([Tbar; Sbar], dstate_old_new),\n    Duplicated(out_now, dout_now),\n    Duplicated(out_old, dout_old),\n    parameters,\n    Const(10*parameters.day)\n)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Now checking dstate_now and dstate_old we see","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show dstate_now_new","category":"page"},{"location":"generated/box/#What-happened?-Enzyme-is-actually-taking-the-computed-gradient-and-acting-on-what-we","page":"Box model","title":"What happened? Enzyme is actually taking the computed gradient and acting on what we","text":"","category":"section"},{"location":"generated/box/#give-as-input-to-dout*now-and-dout*old.-Checking-this,-we-see","page":"Box model","title":"give as input to doutnow and doutold. Checking this, we see","text":"","category":"section"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show 2*dstate_now","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"and they match the new results. This exactly matches what we'd expect to happen since we scaled dout_now by two.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Example 2: Full sensitivity calculations","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Now we want to use Enzyme for a bit more than just a single derivative. Let's say we'd like to understand how sensitive the final temperature of Box One is to the initial salinity of Box Two. That is, given the function","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"J = (100000)^T cdot mathbfx(t_f)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"we want Enzyme to calculate the derivative","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"fracpartial Jpartial mathbfx(0)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"where x(t) is the state of the model at time t. If we think about x(t_f) as solely depending on the initial condition, then this derivative is really","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"fracpartial Jpartial mathbfx(0) = fracpartialpartial mathbfx(0) left( (100000)^T cdot L(ldots(L(mathbfx(0)))) right)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"with L(x(t)) = x(t + dt), i.e. one forward step. One could expand this derivative with the chain rule (and it would be very complicated), but really this is where Enzyme comes in. Each run of autodiff on our forward function is one piece of this big chain rule done for us! We also note that the chain rule goes from the outside in, so we start with the derivative of the forward function at the final state, and work backwards until the initial state. To get Enzyme to do this, we complete the following steps:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Run the forward model and store outputs (in a real ocean model this wouldn't be    feasible and we'd need to use checkpointing)\nCompute the initial derivative from the final state\nUse Enzyme to work backwards until we reach the desired derivative.","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"For simplicity we define a function that takes completes our AD steps","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"function compute_adjoint_values(states_before_smoother, states_after_smoother, M, parameters)\n\n    dout_now = [0.0;0.0;0.0;0.0;0.0;0.0]\n    dout_old = [1.0;0.0;0.0;0.0;0.0;0.0]\n\n    for j = M:-1:1\n\n        dstate_now = zeros(6)\n        dstate_old = zeros(6)\n\n        autodiff(Reverse,\n            one_step_forward,\n            Duplicated(states_before_smoother[j], dstate_now),\n            Duplicated(states_after_smoother[j], dstate_old),\n            Duplicated(zeros(6), dout_now),\n            Duplicated(zeros(6), dout_old),\n            parameters,\n            Const(10*parameters.day)\n        )\n\n        if j == 1\n            return dstate_now, dstate_old\n        end\n\n        dout_now = copy(dstate_now)\n        dout_old = copy(dstate_old)\n\n    end\n\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"First we integrate the model forward:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"M = 10000                       ## Total number of forward steps to take\nTbar = [20.0; 1.0; 1.0]         ## initial temperatures\nSbar = [35.5; 34.5; 34.5]       ## initial salinities\n\nstates_after_smoother, states_before_smoother = integrate(\n    copy([Tbar; Sbar]),\n    copy([Tbar; Sbar]),\n    10*parameters.day,\n    M,\n    parameters\n)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Next, we pass all of our states to the AD function to get back to the desired derivative:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"dstate_now, dstate_old = compute_adjoint_values(\n    states_before_smoother,\n    states_after_smoother,\n    M,\n    parameters\n)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"And we're done! We were interested in sensitivity to the initial salinity of box two, which will live in what we've called dstate_old. Checking this value we see","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show dstate_old[5]","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"As it stands this is just a number, but a good check that Enzyme has computed what we want is to approximate the derivative with a Taylor series. Specifically,","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"J(mathbfx(0) + varepsilon) approx J(mathbfx(0)) +\nvarepsilon fracpartial Jpartial mathbfx(0)","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"and a simple rearrangement yields","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"fracpartial Jpartial mathbfx(0) approx\nfracJ(mathbfx(0) + varepsilon)  - J(mathbfx(0))varepsilon","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Hopefully we see that the analytical values converge close to the one we found with Enzyme:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"# unperturbed final state\nuse_to_check = states_after_smoother[M+1]\n\n# a loop to compute the perturbed final states\ndiffs = []\nstep_sizes = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]\nfor eps in step_sizes\n\n    state_new_smoothed = zeros(6)\n\n    initial_temperature = [20.0; 1.0; 1.0]\n    perturbed_initial_salinity = [35.5; 34.5; 34.5] + [0.0; eps; 0.0]\n\n    state_old = [initial_temperature; perturbed_initial_salinity]\n    state_now = [20.0; 1.0; 1.0; 35.5; 34.5; 34.5]\n\n    for t = 1:M\n\n        rho = compute_density(state_now, parameters)\n        u = compute_transport(rho, parameters)\n        state_new = compute_update(state_now, state_old, u, parameters, 10*parameters.day)\n\n        state_new_smoothed[:] = state_now + parameters.rf_coeff * (state_new - 2.0 * state_now + state_old)\n\n        state_old = state_new_smoothed\n        state_now = state_new\n\n    end\n\n    push!(diffs, (state_old[1] - use_to_check[1])/eps)\n\nend","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"Then checking what we found the derivative to be analytically:","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show diffs","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"which comes very close to our calculated value. We can go further and check the percent difference to see","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"@show abs.(diffs .- dstate_old[5])./dstate_old[5]","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"and we get down to a percent difference on the order of 1e^-5, showing Enzyme calculated the correct derivative. Success!","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"","category":"page"},{"location":"generated/box/","page":"Box model","title":"Box model","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"EditURL = \"https://github.com/EnzymeAD/Enzyme.jl/blob/main/examples/custom_rule.jl\"","category":"page"},{"location":"generated/custom_rule/#Enzyme-custom-rules-tutorial","page":"Custom rules","title":"Enzyme custom rules tutorial","text":"","category":"section"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"The goal of this tutorial is to give a simple example of defining a custom rule with Enzyme. Specifically, our goal will be to write custom rules for the following function f:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"function f(y, x)\n    y .= x.^2\n    return sum(y)\nend","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Our function f populates its first input y with the element-wise square of x. In addition, it returns sum(y) as output. What a sneaky function!","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"In this case, Enzyme can differentiate through f automatically. For example, using forward mode:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"using Enzyme\nx  = [3.0, 1.0]\ndx = [1.0, 0.0]\ny  = [0.0, 0.0]\ndy = [0.0, 0.0]\n\ng(y, x) = f(y, x)^2 # function to differentiate\n\n@show autodiff(Forward, g, Duplicated(y, dy), Duplicated(x, dx)) # derivative of g w.r.t. x[1]\n@show dy; # derivative of y w.r.t. x[1] when g is run\nnothing #hide","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"(See the AutoDiff API tutorial for more information on using autodiff.)","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"But there may be special cases where we need to write a custom rule to help Enzyme out. Let's see how to write a custom rule for f!","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"warning: Don't use custom rules unnecessarily!\nEnzyme can efficiently handle a wide range of constructs, and so a custom rule should only be required in certain special cases. For example, a function may make a foreign call that Enzyme cannot differentiate, or we may have higher-level mathematical knowledge that enables us to write a more efficient rule. Even in these cases, try to make your custom rule encapsulate the minimum possible construct that Enzyme cannot differentiate, rather than expanding the scope of the rule unnecessarily. For pedagogical purposes, we will disregard this principle here and go ahead and write a custom rule for f :)","category":"page"},{"location":"generated/custom_rule/#Defining-our-first-rule","page":"Custom rules","title":"Defining our first rule","text":"","category":"section"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"First, we import the functions EnzymeRules.forward, EnzymeRules.augmented_primal, and EnzymeRules.reverse. We need to overload forward in order to define a custom forward rule, and we need to overload augmented_primal and reverse in order to define a custom reverse rule.","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"import .EnzymeRules: forward, reverse, augmented_primal\nusing .EnzymeRules","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"In this section, we write a simple forward rule to start out:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"function forward(func::Const{typeof(f)}, ::Type{<:Duplicated}, y::Duplicated, x::Duplicated)\n    println(\"Using custom rule!\")\n    ret = func.val(y.val, x.val)\n    y.dval .= 2 .* x.val .* x.dval\n    return Duplicated(ret, sum(y.dval))\nend","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"In the signature of our rule, we have made use of Enzyme's activity annotations. Let's break down each one:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"the Const annotation on f indicates that we accept a function f that does not have a derivative component, which makes sense since f is not a closure with data that could be differentiated.\nthe Duplicated annotation given in the second argument annotates the return value of f. This means that our forward function should return an output of type Duplicated, containing the original output sum(y) and its derivative.\nthe Duplicated annotations for x and y mean that our forward function handles inputs x and y which have been marked as Duplicated. We should update their shadows with their derivative contributions.","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"In the logic of our forward function, we run the original function, populate y.dval (the shadow of y), and finally return a Duplicated for the output as promised. Let's see our rule in action! With the same setup as before:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"x  = [3.0, 1.0]\ndx = [1.0, 0.0]\ny  = [0.0, 0.0]\ndy = [0.0, 0.0]\n\ng(y, x) = f(y, x)^2 # function to differentiate\n\n@show autodiff(Forward, g, Duplicated(y, dy), Duplicated(x, dx)) # derivative of g w.r.t. x[1]\n@show dy; # derivative of y w.r.t. x[1] when g is run\nnothing #hide","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"We see that our custom forward rule has been triggered and gives the same answer as before.","category":"page"},{"location":"generated/custom_rule/#Handling-more-activities","page":"Custom rules","title":"Handling more activities","text":"","category":"section"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Our custom rule applies for the specific set of activities that are annotated for f in the above autodiff call. However, Enzyme has a number of other annotations. Let us consider a particular example, where the output has a DuplicatedNoNeed annotation. This means we are only interested in its derivative, not its value. To squeeze out the last drop of performance, the below rule avoids computing the output of the original function and just computes its derivative.","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"function forward(func::Const{typeof(f)}, ::Type{<:DuplicatedNoNeed}, y::Duplicated, x::Duplicated)\n    println(\"Using custom rule with DuplicatedNoNeed output.\")\n    y.val .= x.val.^2\n    y.dval .= 2 .* x.val .* x.dval\n    return sum(y.dval)\nend","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Our rule is triggered, for example, when we call autodiff directly on f, as the return value's derivative isn't needed:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"x  = [3.0, 1.0]\ndx = [1.0, 0.0]\ny  = [0.0, 0.0]\ndy = [0.0, 0.0]\n\n@show autodiff(Forward, f, Duplicated(y, dy), Duplicated(x, dx)) # derivative of f w.r.t. x[1]\n@show dy; # derivative of y w.r.t. x[1] when f is run\nnothing #hide","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"note: Custom rule dispatch\nWhen multiple custom rules for a function are defined, the correct rule is chosen using Julia's multiple dispatch. In particular, it is important to understand that the custom rule does not determine the activities of the inputs and the return value: rather, Enzyme decides the activity annotations independently, and then dispatches to the custom rule handling the activities, if one exists. If a custom rule is specified for the correct function/argument types, but not the correct activity annotation, a runtime error will be thrown alerting the user to the missing activity rule rather than silently ignoring the rule.\"","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Finally, it may be that either x, y, or the return value are marked as Const. We can in fact handle this case, along with the previous two cases, all together in a single rule:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Base.delete_method.(methods(forward, (Const{typeof(f)}, Vararg{Any}))) # delete our old rules\n\nfunction forward(func::Const{typeof(f)}, RT::Type{<:Union{Const, DuplicatedNoNeed, Duplicated}},\n                 y::Union{Const, Duplicated}, x::Union{Const, Duplicated})\n    println(\"Using our general custom rule!\")\n    y.val .= x.val.^2\n    if !(x isa Const) && !(y isa Const)\n        y.dval .= 2 .* x.val .* x.dval\n    elseif !(y isa Const)\n        y.dval .= 0\n    end\n    dret = !(y isa Const) ? sum(y.dval) : zero(eltype(y.val))\n    if RT <: Const\n        return nothing\n    elseif RT <: DuplicatedNoNeed\n        return dret\n    else\n        return Duplicated(sum(y.val), dret)\n    end\nend","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Let's try out our rule:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"x  = [3.0, 1.0]\ndx = [1.0, 0.0]\ny  = [0.0, 0.0]\ndy = [0.0, 0.0]\n\ng(y, x) = f(y, x)^2 # function to differentiate\n\n@show autodiff(Forward, g, Duplicated(y, dy), Duplicated(x, dx)) # derivative of g w.r.t. x[1]\n@show autodiff(Forward, g, Const(y), Duplicated(x, dx)) # derivative of g w.r.t. x[1], with y annotated Const\n@show autodiff(Forward, g, Const(y), Const(x)); # derivative of g w.r.t. x[1], with x and y annotated Const\nnothing #hide","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Note that there are also exist batched duplicated annotations for forward mode, namely BatchDuplicated and BatchDuplicatedNoNeed, which are not covered in this tutorial.","category":"page"},{"location":"generated/custom_rule/#Defining-a-reverse-mode-rule","page":"Custom rules","title":"Defining a reverse-mode rule","text":"","category":"section"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Let's look at how to write a simple reverse-mode rule! First, we write a method for EnzymeRules.augmented_primal:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"function augmented_primal(config::ConfigWidth{1}, func::Const{typeof(f)}, ::Type{<:Active},\n                          y::Duplicated, x::Duplicated)\n    println(\"In custom augmented primal rule.\")\n    # Compute primal\n    if needs_primal(config)\n        primal = func.val(y.val, x.val)\n    else\n        y.val .= x.val.^2 # y still needs to be mutated even if primal not needed!\n        primal = nothing\n    end\n    # Save x in tape if x will be overwritten\n    if overwritten(config)[3]\n        tape = copy(x.val)\n    else\n        tape = nothing\n    end\n    # Return an AugmentedReturn object with shadow = nothing\n    return AugmentedReturn(primal, nothing, tape)\nend","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Let's unpack our signature for augmented_primal :","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"We accepted a EnzymeRules.Config object with a specified width of 1, which means that our rule does not support batched reverse mode.\nWe annotated f with Const as usual.\nWe dispatched on an Active annotation for the return value. This is a special annotation for scalar values, such as our return value, that indicates that that we care about the value's derivative but we need not explicitly allocate a mutable shadow since it is a scalar value.\nWe annotated x and y with Duplicated, similar to our first simple forward rule.","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Now, let's unpack the body of our augmented_primal rule:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"We checked if the config requires the primal. If not, we need not compute the return value, but we make sure to mutate y in all cases.\nWe checked if x could possibly be overwritten using the Overwritten attribute of EnzymeRules.Config. If so, we save the elements of x on the tape of the returned EnzymeRules.AugmentedReturn object.\nWe return a shadow of nothing since the return value is Active and hence does not need a shadow.","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Now, we write a method for EnzymeRules.reverse:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"function reverse(config::ConfigWidth{1}, func::Const{typeof(f)}, dret::Active, tape,\n                 y::Duplicated, x::Duplicated)\n    println(\"In custom reverse rule.\")\n    # retrieve x value, either from original x or from tape if x may have been overwritten.\n    xval = overwritten(config)[3] ? tape : x.val\n    # accumulate dret into x's shadow. don't assign!\n    x.dval .+= 2 .* xval .* dret.val\n    # also accumulate any derivative in y's shadow into x's shadow.\n    x.dval .+= 2 .* xval .* y.dval\n    y.dval .= 0\n    return (nothing, nothing)\nend","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Let's make a few observations about our reverse rule:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"The activities used in the signature correspond to what we used for augmented_primal.\nHowever, for Active return types such as in this case, we now receive an instance dret of Active for the return type, not just a type annotation, which stores the derivative value for ret (not the original return value!). For the other annotations (e.g. Duplicated), we still receive only the type. In that case, if necessary a reference to the shadow of the output should be placed on the tape in augmented_primal.\nUsing dret.val and y.dval, we accumulate the backpropagated derivatives for x into its shadow x.dval. Note that we have to accumulate from both y.dval and dret.val. This is because in reverse-mode AD we have to sum up the derivatives from all uses: if y was read after our function, we need to consider derivatives from that use as well.\nFinally, we zero-out y's shadow.  This is because y is overwritten within f, so there is no derivative w.r.t. to the y that was originally inputted.","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Finally, let's see our reverse rule in action!","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"x  = [3.0, 1.0]\ndx = [0.0, 0.0]\ny  = [0.0, 0.0]\ndy = [0.0, 0.0]\n\ng(y, x) = f(y, x)^2\n\nautodiff(Reverse, g, Duplicated(y, dy), Duplicated(x, dx))\n@show dx # derivative of g w.r.t. x\n@show dy; # derivative of g w.r.t. y\nnothing #hide","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Let's also try a function which mutates x after running f, and also uses y directly rather than only ret after running f (but ultimately gives the same result as above):","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"function h(y, x)\n    ret = f(y, x)\n    x .= x.^2\n    return ret * sum(y)\nend\n\nx  = [3.0, 1.0]\ny  = [0.0, 0.0]\ndx .= 0\ndy .= 0\n\nautodiff(Reverse, h, Duplicated(y, dy), Duplicated(x, dx))\n@show dx # derivative of h w.r.t. x\n@show dy; # derivative of h w.r.t. y\nnothing #hide","category":"page"},{"location":"generated/custom_rule/#Marking-functions-inactive","page":"Custom rules","title":"Marking functions inactive","text":"","category":"section"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"If we want to tell Enzyme that the function call does not affect the differentiation result in any form (i.e. not by side effects or through its return values), we can simply use EnzymeRules.inactive. So long as there exists a matching dispatch to EnzymeRules.inactive, the function will be considered inactive. For example:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"printhi() = println(\"Hi!\")\nEnzymeRules.inactive(::typeof(printhi), args...) = nothing\n\nfunction k(x)\n    printhi()\n    return x^2\nend\n\nautodiff(Forward, k, Duplicated(2.0, 1.0))","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"Or for a case where we incorrectly mark a function inactive:","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"double(x) = 2*x\nEnzymeRules.inactive(::typeof(double), args...) = nothing\n\nautodiff(Forward, x -> x + double(x), Duplicated(2.0, 1.0)) # mathematically should be 3.0, inactive rule causes it to be 1.0","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"","category":"page"},{"location":"generated/custom_rule/","page":"Custom rules","title":"Custom rules","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Enzyme\nDocTestSetup = quote\n    using Enzyme\nend","category":"page"},{"location":"#Enzyme","page":"Home","title":"Enzyme","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Enzyme.jl, the Julia bindings for Enzyme.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Enzyme performs automatic differentiation (AD) of statically analyzable LLVM. It is highly-efficient and its ability perform AD on optimized code allows Enzyme to meet or exceed the performance of state-of-the-art AD tools.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Enzyme.jl can be installed in the usual way Julia packages are installed:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Enzyme","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Enzyme binary dependencies will be installed automatically via Julia's binary actifact system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Enzyme.jl API revolves around the function autodiff, see its documentation for details and a usage example. Also see Implementing pullbacks on how to use Enzyme.jl to implement back-propagation for functions with non-scalar results.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> rosenbrock(x, y) = (1.0 - x)^2 + 100.0 * (y - x^2)^2\nrosenbrock (generic function with 1 method)\n\njulia> rosenbrock_inp(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nrosenbrock_inp (generic function with 1 method)","category":"page"},{"location":"#Reverse-mode","page":"Home","title":"Reverse mode","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The return value of reverse mode is a tuple that contains as a first value the derivative value of the active inputs and optionally the primal return value.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Reverse, rosenbrock, Active, Active(1.0), Active(2.0))\n((-400.0, 200.0),)\n\njulia> autodiff(ReverseWithPrimal, rosenbrock, Active, Active(1.0), Active(2.0))\n((-400.0, 200.0), 100.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> x = [1.0, 2.0]\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> dx = [0.0, 0.0]\n2-element Vector{Float64}:\n 0.0\n 0.0\n\njulia> autodiff(Reverse, rosenbrock_inp, Active, Duplicated(x, dx))\n((nothing,),)\n\njulia> dx\n2-element Vector{Float64}:\n -400.0\n  200.0","category":"page"},{"location":"","page":"Home","title":"Home","text":"Both the inplace and \"normal\" variant return the gradient. The difference is that with Active the gradient is returned and with Duplicated the gradient is accumulated in place.","category":"page"},{"location":"#Forward-mode","page":"Home","title":"Forward mode","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The return value of forward mode with a Duplicated return is a tuple containing as the first value the primal return value and as the second value the derivative.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In forward mode Duplicated(x, 0.0) is equivalent to Const(x), except that we can perform more optimizations for Const.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Forward, rosenbrock, Duplicated, Const(1.0), Duplicated(3.0, 1.0))\n(400.0, 400.0)\n\njulia> autodiff(Forward, rosenbrock, Duplicated, Duplicated(1.0, 1.0), Const(3.0))\n(400.0, -800.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Of note, when we seed both arguments at once the tangent return is the sum of both.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Forward, rosenbrock, Duplicated, Duplicated(1.0, 1.0), Duplicated(3.0, 1.0))\n(400.0, -400.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can also use forward mode with our inplace method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> x = [1.0, 3.0]\n2-element Vector{Float64}:\n 1.0\n 3.0\n\njulia> dx=[1.0, 1.0]\n2-element Vector{Float64}:\n 1.0\n 1.0\n\njulia> autodiff(Forward, rosenbrock_inp, Duplicated, Duplicated(x, dx))\n(400.0, -400.0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note the seeding through dx.","category":"page"},{"location":"#Vector-forward-mode","page":"Home","title":"Vector forward mode","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We can also use vector mode to calculate both derivatives at once.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> autodiff(Forward, rosenbrock, BatchDuplicated, BatchDuplicated(1.0, (1.0, 0.0)), BatchDuplicated(3.0, (0.0, 1.0)))\n(400.0, (var\"1\" = -800.0, var\"2\" = 400.0))\n\njulia> x = [1.0, 3.0]\n2-element Vector{Float64}:\n 1.0\n 3.0\n\njulia> dx_1 = [1.0, 0.0]; dx_2 = [0.0, 1.0];\n\njulia> autodiff(Forward, rosenbrock_inp, BatchDuplicated, BatchDuplicated(x, (dx_1, dx_2)))\n(400.0, (var\"1\" = -800.0, var\"2\" = 400.0))","category":"page"},{"location":"#Caveats-/-Known-issues","page":"Home","title":"Caveats / Known-issues","text":"","category":"section"},{"location":"#Activity-of-temporary-storage","page":"Home","title":"Activity of temporary storage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you have pass any temporary storage which may be involved in an active computation to a function you want to differentiate, you must also pass in a duplicated temporary storage for use in computing the derivatives. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"function f(x, tmp, n)\n   tmp[1] = 1\n   for i in 1:n\n\t   tmp[1] *= x\n   end\n   tmp[1]\nend\n\n# Incorrect [ returns (0.0,) ]\nEnzyme.autodiff(f, Active(1.2), Const(Vector{Float64}(undef, 1)), Const(5))\n\n# Correct [ returns (10.367999999999999,) == 1.2^4 * 5 ]\nEnzyme.autodiff(f, Active(1.2), Duplicated(Vector{Float64}(undef, 1), Vector{Float64}(undef, 1)), Const(5))","category":"page"},{"location":"#CUDA.jl-support","page":"Home","title":"CUDA.jl support","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CUDA.jl is only support on Julia v1.7.0 and onwards. On 1.6 attempting to differentiate CUDA kernel functions, will not use device overloads correctly and thus return fundamentally wrong results.","category":"page"},{"location":"#Sparse-Arrays","page":"Home","title":"Sparse Arrays","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"At the momment there is limited support for sparse linear algebra operations. Sparse arrays may be used, but care must be taken because backing arrays drop zeros in Julia (unless told not to).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SparseArrays\n\na=sparse([2.0])\nf(a)=sum(a)\n\n# Incorrect: SparseMatrixCSC drops explicit zeros\n# returns 1-element SparseVector{Float64, Int64} with 0 stored entries\nda=sparse([0.0])\n\n# Correct: Prevent SparseMatrixCSC from dropping zeros\n# returns 1-element SparseVector{Float64, Int64} with 1 stored entry:\n#  [1]  =  1.0\nda=sparsevec([1],[0.0])\n\nEnzyme.autodiff(Reverse,f,Active,Duplicated(a,da))\n@show da","category":"page"}]
}
